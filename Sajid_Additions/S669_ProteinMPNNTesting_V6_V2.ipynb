{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S669_ProteinMPNNTesting_V6_V2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e2e7c57f453340baaa195441e2ce38aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4ee451d7f76430fb16cdf5282198629",
              "IPY_MODEL_2ccd6a5b15204e4e921158b691d66a49",
              "IPY_MODEL_b9ce1ed852fe4f9799c4b3df0e2de121"
            ],
            "layout": "IPY_MODEL_895adf35bd9540a8a11118e4fa85b36e"
          }
        },
        "f4ee451d7f76430fb16cdf5282198629": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b9553b37a24a75b28a5eeb1c25cdac",
            "placeholder": "​",
            "style": "IPY_MODEL_c2ba6bde3068456ab5eb0ad721bf3261",
            "value": ""
          }
        },
        "2ccd6a5b15204e4e921158b691d66a49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea1768949328415ab787707e133c4b54",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c1fafee76dd4d1a88e1f3e0957f3e6e",
            "value": 1
          }
        },
        "b9ce1ed852fe4f9799c4b3df0e2de121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8433f763095b40a3bf777826eb02d4ed",
            "placeholder": "​",
            "style": "IPY_MODEL_4fe2f57b0bdc436c819b874906a95064",
            "value": " 669/? [00:00&lt;00:00, 22508.96it/s]"
          }
        },
        "895adf35bd9540a8a11118e4fa85b36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5b9553b37a24a75b28a5eeb1c25cdac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ba6bde3068456ab5eb0ad721bf3261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea1768949328415ab787707e133c4b54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0c1fafee76dd4d1a88e1f3e0957f3e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8433f763095b40a3bf777826eb02d4ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fe2f57b0bdc436c819b874906a95064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa090cadb889484eb2f1b20727389fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f318cf0d34643e7bb114058a93b4014",
              "IPY_MODEL_9061c05f5463497c9212f9d26e75f2fa",
              "IPY_MODEL_f00cd0764b2f44e280e4d50d0258601e"
            ],
            "layout": "IPY_MODEL_e62490ffcdc94d5497761260cb6f7af7"
          }
        },
        "4f318cf0d34643e7bb114058a93b4014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f558d33793cc40bba9e4d124d65a5a43",
            "placeholder": "​",
            "style": "IPY_MODEL_69cfeb5faf1b4ec7a93b17e1e654b5db",
            "value": "100%"
          }
        },
        "9061c05f5463497c9212f9d26e75f2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60dd91e47a254e6abf79d1e590418d2b",
            "max": 93,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_559c046103f24a14893bfb163c0c9afa",
            "value": 93
          }
        },
        "f00cd0764b2f44e280e4d50d0258601e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d99c0ff7ec0e4fa4b34096db3e733533",
            "placeholder": "​",
            "style": "IPY_MODEL_13a3bd48f9d444c98d62f81d0ac6aecd",
            "value": " 93/93 [00:03&lt;00:00, 37.58it/s]"
          }
        },
        "e62490ffcdc94d5497761260cb6f7af7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f558d33793cc40bba9e4d124d65a5a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cfeb5faf1b4ec7a93b17e1e654b5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60dd91e47a254e6abf79d1e590418d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "559c046103f24a14893bfb163c0c9afa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d99c0ff7ec0e4fa4b34096db3e733533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a3bd48f9d444c98d62f81d0ac6aecd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_wbcvVBv0QJ",
        "outputId": "ca45313b-1c95-4d75-c0bc-cc9b6b061793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "import sys \n",
        "installation_path = \"/content/drive/MyDrive/Colab_Installations_V2\"\n",
        "# The path is being modified so that everything installed in the installation path can now be used without re-installing (in this case, I just need biopython)\n",
        "sys.path.insert(0,installation_path)\n",
        "protein_mpnn_path = \"/content/drive/MyDrive/Protein_MPNN_Digging/ProteinMPNN/vanilla_proteinmpnn\"\n",
        "sys.path.insert(0,protein_mpnn_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Protein_MPNN_Digging"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgTOgsabxBaC",
        "outputId": "69fffafb-3140-4595-8523-41b8025a91b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1KHc6SLCFDWefngMT266PS74JQMzqN06K/Protein_MPNN_Digging\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import warnings\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "from Bio.PDB import *\n",
        "\n",
        "device = torch.device(\"cuda\" if (torch.cuda.is_available()) else \"cpu\")"
      ],
      "metadata": {
        "id": "jX5ScMeGyLcy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from Bio.PDB.Polypeptide import *\n",
        "from string import ascii_uppercase"
      ],
      "metadata": {
        "id": "NBjszWagtiYL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = os.path.join(protein_mpnn_path,\"vanilla_model_weights\")\n",
        "model_name = \"v_48_020\"\n",
        "checkpoint_path = os.path.join(weights_path,model_name+\".pt\")"
      ],
      "metadata": {
        "id": "Z6ZHe2IIyy1G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, load and dig into the checkpoint object\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device) "
      ],
      "metadata": {
        "id": "JPE_pX8tzdUO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(checkpoint[\"num_edges\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DD1hpMLk2-y",
        "outputId": "3e6f500b-d00f-4aeb-bbc3-b3365161e7ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import json, time, os, sys, glob\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import random_split, Subset\n",
        "\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "#A number of functions/classes are adopted from: https://github.com/jingraham/neurips19-graph-protein-design\n",
        "\n",
        "def _scores(S, log_probs, mask):\n",
        "    \"\"\" Negative log probabilities \"\"\"\n",
        "    criterion = torch.nn.NLLLoss(reduction='none')\n",
        "    loss = criterion(\n",
        "        log_probs.contiguous().view(-1,log_probs.size(-1)),\n",
        "        S.contiguous().view(-1)\n",
        "    ).view(S.size())\n",
        "    # The designable positions have mask set to 1.0, so this function seems to be returning the average score for the designable positions\n",
        "    scores = torch.sum(loss * mask, dim=-1) / torch.sum(mask, dim=-1)\n",
        "    return scores\n",
        "\n",
        "def _S_to_seq(S, mask):\n",
        "    # This is the decoding order\n",
        "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "    seq = ''.join([alphabet[c] for c, m in zip(S.tolist(), mask.tolist()) if m > 0])\n",
        "    return seq\n",
        "\n",
        "def parse_PDB_biounits(x, atoms=['N','CA','C'], chain=None):\n",
        "  '''\n",
        "  input:  x = PDB filename\n",
        "          atoms = atoms to extract (optional)\n",
        "  output: (length, atoms, coords=(x,y,z)), sequence\n",
        "  '''\n",
        "\n",
        "  alpha_1 = list(\"ARNDCQEGHILKMFPSTWYV-\")\n",
        "  states = len(alpha_1)\n",
        "  alpha_3 = ['ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE',\n",
        "             'LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL','GAP']\n",
        "  \n",
        "  # The following dictionaries are mapping from one-letter to 0-20 index,\n",
        "  # three-letter to 0-20 index,\n",
        "  # 0-20 index to one-letter,\n",
        "  # one-letter to three-letter, and vice-versa \n",
        "  aa_1_N = {a:n for n,a in enumerate(alpha_1)}\n",
        "  aa_3_N = {a:n for n,a in enumerate(alpha_3)}\n",
        "  aa_N_1 = {n:a for n,a in enumerate(alpha_1)}\n",
        "  aa_1_3 = {a:b for a,b in zip(alpha_1,alpha_3)}\n",
        "  aa_3_1 = {b:a for a,b in zip(alpha_1,alpha_3)}\n",
        "  \n",
        "  def AA_to_N(x):\n",
        "    # [\"ARND\"] -> [[0,1,2,3]]\n",
        "    x = np.array(x);\n",
        "    if x.ndim == 0: x = x[None]\n",
        "    return [[aa_1_N.get(a, states-1) for a in y] for y in x]\n",
        "  \n",
        "  def N_to_AA(x):\n",
        "    # [[0,1,2,3]] -> [\"ARND\"]\n",
        "    x = np.array(x);\n",
        "    if x.ndim == 1: x = x[None]\n",
        "    return [\"\".join([aa_N_1.get(a,\"-\") for a in y]) for y in x]\n",
        "\n",
        "  xyz,seq,min_resn,max_resn = {},{},1e6,-1e6\n",
        "  for line in open(x,\"rb\"):\n",
        "    line = line.decode(\"utf-8\",\"ignore\").rstrip()\n",
        "\n",
        "    if line[:6] == \"HETATM\" and line[17:17+3] == \"MSE\":\n",
        "      line = line.replace(\"HETATM\",\"ATOM  \")\n",
        "      line = line.replace(\"MSE\",\"MET\")\n",
        "\n",
        "    if line[:4] == \"ATOM\":\n",
        "      ch = line[21:22]\n",
        "      # If the input chain is not in the PDB file, which can be the case if the target chains are named differently in the runner script,\n",
        "      # this line will cause the output to have literally no information, this is the case for integer named chains\n",
        "      # that does not mean that this line is not doing its job correctly, this is just a constraint that input chain names and \n",
        "      # chain names in the PDB file have to be congruent\n",
        "      if ch == chain or chain is None:\n",
        "        atom = line[12:12+4].strip()\n",
        "        resi = line[17:17+3]\n",
        "        resn = line[22:22+5].strip()\n",
        "        x,y,z = [float(line[i:(i+8)]) for i in [30,38,46]]\n",
        "\n",
        "        if resn[-1].isalpha(): \n",
        "            resa,resn = resn[-1],int(resn[:-1])-1\n",
        "        else: \n",
        "            resa,resn = \"\",int(resn)-1\n",
        "#         resn = int(resn)\n",
        "        if resn < min_resn: \n",
        "            min_resn = resn\n",
        "        if resn > max_resn: \n",
        "            max_resn = resn\n",
        "        if resn not in xyz: \n",
        "            xyz[resn] = {}\n",
        "        if resa not in xyz[resn]: \n",
        "            xyz[resn][resa] = {}\n",
        "        if resn not in seq: \n",
        "            seq[resn] = {}\n",
        "        if resa not in seq[resn]: \n",
        "            seq[resn][resa] = resi\n",
        "\n",
        "        if atom not in xyz[resn][resa]:\n",
        "          xyz[resn][resa][atom] = np.array([x,y,z])\n",
        "\n",
        "  # convert to numpy arrays, fill in missing values\n",
        "  seq_,xyz_ = [],[]\n",
        "  try:\n",
        "      for resn in range(min_resn,max_resn+1):\n",
        "        if resn in seq:\n",
        "          for k in sorted(seq[resn]): seq_.append(aa_3_N.get(seq[resn][k],20))\n",
        "        else: seq_.append(20)\n",
        "        if resn in xyz:\n",
        "          for k in sorted(xyz[resn]):\n",
        "            for atom in atoms:\n",
        "              if atom in xyz[resn][k]: xyz_.append(xyz[resn][k][atom])\n",
        "              else: xyz_.append(np.full(3,np.nan))\n",
        "        else:\n",
        "          for atom in atoms: xyz_.append(np.full(3,np.nan))\n",
        "      return np.array(xyz_).reshape(-1,len(atoms),3), N_to_AA(np.array(seq_))\n",
        "  except TypeError:\n",
        "      return 'no_chain', 'no_chain'\n",
        "\n",
        "### calling signature\n",
        "# pdb_dict_list = parse_PDB(pdb_path, input_chain_list=chain_list)\n",
        "def parse_PDB(path_to_pdb, input_chain_list=None):\n",
        "    c=0\n",
        "    pdb_dict_list = []\n",
        "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
        "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
        "    chain_alphabet = init_alphabet + extra_alphabet\n",
        "     \n",
        "    if input_chain_list:\n",
        "        chain_alphabet = input_chain_list  \n",
        " \n",
        "\n",
        "    biounit_names = [path_to_pdb]\n",
        "    # Each of the biounits is a separate PDB file, so for running with a single PDB file like from colab, this loop will be executed only once\n",
        "    for biounit in biounit_names:\n",
        "        my_dict = {}\n",
        "        s = 0\n",
        "        concat_seq = ''\n",
        "        concat_N = []\n",
        "        concat_CA = []\n",
        "        concat_C = []\n",
        "        concat_O = []\n",
        "        concat_mask = []\n",
        "        coords_dict = {} \n",
        "        # This loop will be executed only once for single chain DDG type cases\n",
        "        for letter in chain_alphabet:\n",
        "            xyz, seq = parse_PDB_biounits(biounit, atoms=['N','CA','C','O'], chain=letter)\n",
        "            if type(xyz) != str:\n",
        "                concat_seq += seq[0]\n",
        "                my_dict['seq_chain_'+letter]=seq[0]\n",
        "                coords_dict_chain = {}\n",
        "                coords_dict_chain['N_chain_'+letter]=xyz[:,0,:].tolist()\n",
        "                coords_dict_chain['CA_chain_'+letter]=xyz[:,1,:].tolist()\n",
        "                coords_dict_chain['C_chain_'+letter]=xyz[:,2,:].tolist()\n",
        "                coords_dict_chain['O_chain_'+letter]=xyz[:,3,:].tolist()\n",
        "                my_dict['coords_chain_'+letter]=coords_dict_chain\n",
        "                s += 1\n",
        "        fi = biounit.rfind(\"/\")\n",
        "        my_dict['name']=biounit[(fi+1):-4]\n",
        "        my_dict['num_of_chains'] = s\n",
        "        my_dict['seq'] = concat_seq\n",
        "        if s <= len(chain_alphabet):\n",
        "            pdb_dict_list.append(my_dict)\n",
        "            c+=1\n",
        "    return pdb_dict_list\n",
        "\n",
        "\n",
        "# X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta \n",
        "# = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
        "# fixed_pos_list = fixed_position_dict[b['name']][letter]\n",
        "# The trick will be to populate this fixed_position_dict from the calling function, and \n",
        "def tied_featurize(batch, device, chain_dict, fixed_position_dict=None, omit_AA_dict=None, tied_positions_dict=None, pssm_dict=None, bias_by_res_dict=None):\n",
        "    \"\"\" Pack and pad batch into torch tensors \"\"\"\n",
        "    alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "    B = len(batch)\n",
        "    lengths = np.array([len(b['seq']) for b in batch], dtype=np.int32) #sum of chain seq lengths\n",
        "    L_max = max([len(b['seq']) for b in batch])\n",
        "    X = np.zeros([B, L_max, 4, 3])\n",
        "    residue_idx = -100*np.ones([B, L_max], dtype=np.int32)\n",
        "    # This \"chain_M\" is the variable of interest for controlling which positions will be fixed vs. which will be designed\n",
        "    # For scoring function-based uses, I intend on sending the sequences one by one for not caring about the slow speed\n",
        "    # Therefore, B will be == 1\n",
        "    # So, for now, I just need to somehow manipulate the indexes corresponding to L_max which will be equal to the length of the single sequence as a consequence\n",
        "    chain_M = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
        "    pssm_coef_all = np.zeros([B, L_max], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
        "    pssm_bias_all = np.zeros([B, L_max, 21], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
        "    pssm_log_odds_all = 10000.0*np.ones([B, L_max, 21], dtype=np.float32) #1.0 for the bits that need to be predicted\n",
        "    # This \"chain_M_pos\" is the variable of interest for controlling which positions will be fixed vs. which will be designed\n",
        "    # For scoring function-based uses, I intend on sending the sequences one by one for not caring about the slow speed\n",
        "    # Therefore, B will be == 1\n",
        "    # So, for now, I just need to somehow manipulate the indexes corresponding to L_max which will be equal to the length of single sequence as a consequence\n",
        "    chain_M_pos = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
        "    bias_by_res_all = np.zeros([B, L_max, 21], dtype=np.float32)\n",
        "    # This \"chain_encoding_all\" is the variable of interest for controlling which positions will be fixed vs. which will be designed\n",
        "    # For scoring function-based uses, I intend on sending the sequences one by one for not caring about the slow speed\n",
        "    # Therefore, B will be == 1\n",
        "    # So, for now, I just need to somehow manipulate the indexes corresponding to L_max which will be equal to the length of single sequence as a consequence\n",
        "    chain_encoding_all = np.zeros([B, L_max], dtype=np.int32) #1.0 for the bits that need to be predicted\n",
        "    S = np.zeros([B, L_max], dtype=np.int32)\n",
        "    omit_AA_mask = np.zeros([B, L_max, len(alphabet)], dtype=np.int32)\n",
        "    # Build the batch\n",
        "    letter_list_list = []\n",
        "    visible_list_list = []\n",
        "    masked_list_list = []\n",
        "    masked_chain_length_list_list = []\n",
        "    tied_pos_list_of_lists_list = []\n",
        "    #shuffle all chains before the main loop\n",
        "    for i, b in enumerate(batch):\n",
        "        # for my current energy function like usecase, the code will reach \"if and not else\" because chain_dict will not be None\n",
        "        if chain_dict != None:\n",
        "            ### Calling function argument assignment START\n",
        "            # chain_id_dict[pdb_dict_list[0]['name']] = (designed_chain_list, fixed_chain_list)\n",
        "            ### Calling function argument assignment END\n",
        "            masked_chains, visible_chains = chain_dict[b['name']] #masked_chains a list of chain letters to predict [A, D, F]\n",
        "        else:\n",
        "            masked_chains = [item[-1:] for item in list(b) if item[:10]=='seq_chain_']\n",
        "            visible_chains = []\n",
        "        num_chains = b['num_of_chains']\n",
        "        all_chains = masked_chains + visible_chains\n",
        "        #random.shuffle(all_chains)\n",
        "    # This for loop can be ignored since it will be executed only once in my single-chain or single-chain-at-a-time implementation\n",
        "    for i, b in enumerate(batch):\n",
        "        mask_dict = {}\n",
        "        a = 0\n",
        "        x_chain_list = []\n",
        "        chain_mask_list = []\n",
        "        # \"chain_seq_list\" will contain string format sequences of all the chains both fixed and designable \n",
        "        chain_seq_list = []\n",
        "        chain_encoding_list = []\n",
        "        c = 1\n",
        "        # \"letter_list\" will contain names of all the chains both fixed and designable\n",
        "        letter_list = []\n",
        "        global_idx_start_list = [0]\n",
        "        # \"visible_list\" will contain names of the fixed chains \n",
        "        visible_list = []\n",
        "        # \"masked_list\" will contain names of the designable chains\n",
        "        masked_list = []\n",
        "        masked_chain_length_list = []\n",
        "        fixed_position_mask_list = []\n",
        "        omit_AA_mask_list = []\n",
        "        pssm_coef_list = []\n",
        "        pssm_bias_list = []\n",
        "        pssm_log_odds_list = []\n",
        "        bias_by_res_list = []\n",
        "        l0 = 0\n",
        "        l1 = 0\n",
        "        # This loop will also be executed once for my single chain case,\n",
        "        # and since the same chain has both designable and fixed positions, the codes insides both of the if \n",
        "        # statements will be executed\n",
        "        for step, letter in enumerate(all_chains):\n",
        "            if letter in visible_chains:\n",
        "                letter_list.append(letter)\n",
        "                visible_list.append(letter)\n",
        "                chain_seq = b[f'seq_chain_{letter}']\n",
        "                chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])\n",
        "                chain_length = len(chain_seq)\n",
        "                global_idx_start_list.append(global_idx_start_list[-1]+chain_length)\n",
        "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
        "                # the \"chain_mask\" varies between fixed and designable chains (1.0 for designable chains which are maxed)\n",
        "                chain_mask = np.zeros(chain_length) #0.0 for visible chains\n",
        "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
        "                x_chain_list.append(x_chain)\n",
        "                chain_mask_list.append(chain_mask)\n",
        "                chain_seq_list.append(chain_seq)\n",
        "                # \"chain_encoding_list\" contains numpy arrays corresponding to chains (each array corresponds to one chain),\n",
        "                # where all elements of the same array is the same value, which is equal to the index of the chain the it corresponds to\n",
        "                # by index, I mean index of the different numpy arrays annotating the chains\n",
        "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
        "                # l0 points at the starting of the current chain and l1 points after the ending of the current chain\n",
        "                l1 += chain_length\n",
        "                # the only value i will have is 0 since it will be executed only once in my single-chain or single-chain-at-a-time implementation\n",
        "                # seems like the chains are separated by  \n",
        "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
        "                l0 += chain_length\n",
        "                c+=1\n",
        "                # The following variables are numpy arrays with entries corresponding to every position in the sequence\n",
        "                # appending these numpy arrays to a list indicates that the chains are added one after one\n",
        "                # same thing goes for the chain_mask and chain_seq variables declared above\n",
        "                # In code-block below in this cell, these lists of numpy arrays are going through np.concatenate(), which is creating\n",
        "                # the final numpy arrays containing co-ordinates, sequence identity, fixed position, masked position, PSSM bias, and everything\n",
        "                # required to pass the sequences through the model\n",
        "                ### START\n",
        "                fixed_position_mask = np.ones(chain_length)\n",
        "                fixed_position_mask_list.append(fixed_position_mask)\n",
        "                # The omit_AA_mask, pssm_coef, pssm_bias, \"bias_by_res_list\", all these numpy arrays are zero for the fixed positions\n",
        "                # since these positions are used as it is, while for the masked_positions, these values can get activated\n",
        "                # which is why the next if statement has several extra lines manipulating these variables according to the amount of information passed \n",
        "                omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)\n",
        "                omit_AA_mask_list.append(omit_AA_mask_temp)\n",
        "                pssm_coef = np.zeros(chain_length)\n",
        "                pssm_bias = np.zeros([chain_length, 21])\n",
        "                pssm_log_odds = 10000.0*np.ones([chain_length, 21])\n",
        "                pssm_coef_list.append(pssm_coef)\n",
        "                pssm_bias_list.append(pssm_bias)\n",
        "                pssm_log_odds_list.append(pssm_log_odds)\n",
        "                bias_by_res_list.append(np.zeros([chain_length, 21]))\n",
        "                ### END\n",
        "            if letter in masked_chains:\n",
        "                masked_list.append(letter)\n",
        "                letter_list.append(letter)\n",
        "                chain_seq = b[f'seq_chain_{letter}']\n",
        "                chain_seq = ''.join([a if a!='-' else 'X' for a in chain_seq])\n",
        "                chain_length = len(chain_seq)\n",
        "                global_idx_start_list.append(global_idx_start_list[-1]+chain_length)\n",
        "                masked_chain_length_list.append(chain_length)\n",
        "                chain_coords = b[f'coords_chain_{letter}'] #this is a dictionary\n",
        "                chain_mask = np.ones(chain_length) #1.0 for masked\n",
        "                x_chain = np.stack([chain_coords[c] for c in [f'N_chain_{letter}', f'CA_chain_{letter}', f'C_chain_{letter}', f'O_chain_{letter}']], 1) #[chain_lenght,4,3]\n",
        "                x_chain_list.append(x_chain)\n",
        "                chain_mask_list.append(chain_mask)\n",
        "                chain_seq_list.append(chain_seq)\n",
        "                chain_encoding_list.append(c*np.ones(np.array(chain_mask).shape[0]))\n",
        "                l1 += chain_length\n",
        "                residue_idx[i, l0:l1] = 100*(c-1)+np.arange(l0, l1)\n",
        "                l0 += chain_length\n",
        "                c+=1\n",
        "                fixed_position_mask = np.ones(chain_length)\n",
        "                if fixed_position_dict!=None:\n",
        "                    fixed_pos_list = fixed_position_dict[b['name']][letter]\n",
        "                    if fixed_pos_list:\n",
        "                        # seems like \"fixed_pos_list\"  can be an 1-indexed integer list corresponding to positions in \"chain_seq\"\n",
        "                        # this thing ultimately controls which positions in the designable chain will be masked, which is why the fixed \n",
        "                        # positions are set to 0.0 since those positions will not be maxed (1 if maxed, 0 if not maxed)\n",
        "                        fixed_position_mask[np.array(fixed_pos_list)-1] = 0.0\n",
        "                fixed_position_mask_list.append(fixed_position_mask)\n",
        "                omit_AA_mask_temp = np.zeros([chain_length, len(alphabet)], np.int32)\n",
        "                # For my current energy function like usecase, \"omit_AA_dict\" will be None, so the following loop can be ignored\n",
        "                if omit_AA_dict!=None:\n",
        "                    for item in omit_AA_dict[b['name']][letter]:\n",
        "                        idx_AA = np.array(item[0])-1\n",
        "                        AA_idx = np.array([np.argwhere(np.array(list(alphabet))== AA)[0][0] for AA in item[1]]).repeat(idx_AA.shape[0])\n",
        "                        idx_ = np.array([[a, b] for a in idx_AA for b in AA_idx])\n",
        "                        omit_AA_mask_temp[idx_[:,0], idx_[:,1]] = 1\n",
        "                omit_AA_mask_list.append(omit_AA_mask_temp)\n",
        "                pssm_coef = np.zeros(chain_length)\n",
        "                pssm_bias = np.zeros([chain_length, 21])\n",
        "                pssm_log_odds = 10000.0*np.ones([chain_length, 21])\n",
        "                if pssm_dict:\n",
        "                    if pssm_dict[b['name']][letter]:\n",
        "                        pssm_coef = pssm_dict[b['name']][letter]['pssm_coef']\n",
        "                        pssm_bias = pssm_dict[b['name']][letter]['pssm_bias']\n",
        "                        pssm_log_odds = pssm_dict[b['name']][letter]['pssm_log_odds']\n",
        "                pssm_coef_list.append(pssm_coef)\n",
        "                pssm_bias_list.append(pssm_bias)\n",
        "                pssm_log_odds_list.append(pssm_log_odds)\n",
        "                if bias_by_res_dict:\n",
        "                    bias_by_res_list.append(bias_by_res_dict[b['name']][letter])\n",
        "                else:\n",
        "                    bias_by_res_list.append(np.zeros([chain_length, 21]))\n",
        "\n",
        "        ### TIED position START\n",
        "        # Since there will technically be no tied positions for my single chain energy-based usecase for now,\n",
        "        # I do not need to dig into this part of the code\n",
        "        letter_list_np = np.array(letter_list)\n",
        "        tied_pos_list_of_lists = []\n",
        "        tied_beta = np.ones(L_max)\n",
        "        if tied_positions_dict!=None:\n",
        "            tied_pos_list = tied_positions_dict[b['name']]\n",
        "            if tied_pos_list:\n",
        "                set_chains_tied = set(list(itertools.chain(*[list(item) for item in tied_pos_list])))\n",
        "                for tied_item in tied_pos_list:\n",
        "                    one_list = []\n",
        "                    for k, v in tied_item.items():\n",
        "                        start_idx = global_idx_start_list[np.argwhere(letter_list_np == k)[0][0]]\n",
        "                        if isinstance(v[0], list):\n",
        "                            for v_count in range(len(v[0])):\n",
        "                                one_list.append(start_idx+v[0][v_count]-1)#make 0 to be the first\n",
        "                                tied_beta[start_idx+v[0][v_count]-1] = v[1][v_count]\n",
        "                        else:\n",
        "                            for v_ in v:\n",
        "                                one_list.append(start_idx+v_-1)#make 0 to be the first\n",
        "                    tied_pos_list_of_lists.append(one_list)\n",
        "        tied_pos_list_of_lists_list.append(tied_pos_list_of_lists)\n",
        "        ### TIED position END\n",
        " \n",
        "        # Interestingly, although the backbone atom coordinates are used for generating edge features,\n",
        "        # the \"x\" in the following line contains the coodinates of the backbone atoms \n",
        "        x = np.concatenate(x_chain_list,0) #[L, 4, 3]\n",
        "        # \"all_sequence\" is a string where all the chain sequences have been put one after another\n",
        "        all_sequence = \"\".join(chain_seq_list)\n",
        "        # This \"chain_mask_list\" and \"m_pos\" below are the variables of interest if these actually contain full information regarding the\n",
        "        # fixed vs. variable positions definitions \n",
        "        # consequently, since these are concatenated numpy arrays of numpy arrays inside the lists \"chain_mask_list\" and \"fixed_position_mask_list\",\n",
        "        # when those lists are populated in the above code-block with binary numpy arrays \"fixed_position_mask\" and \"fixed_position_mask\" corresponding to \n",
        "        # each of the chains,\n",
        "        # that is where all the controlling needs to be done from\n",
        "        m = np.concatenate(chain_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
        "        # \"chain_encoding_list\" contains numpy arrays corresponding to chains (each array corresponds to one chain),\n",
        "        # where all elements of the same array is the same value, which is equal to the index of the chain the it corresponds to\n",
        "        # by index, I mean index of the different numpy arrays annotating the chains\n",
        "        chain_encoding = np.concatenate(chain_encoding_list,0)\n",
        "        m_pos = np.concatenate(fixed_position_mask_list,0) #[L,], 1.0 for places that need to be predicted\n",
        "\n",
        "        pssm_coef_ = np.concatenate(pssm_coef_list,0) #[L,], 1.0 for places that need to be predicted\n",
        "        pssm_bias_ = np.concatenate(pssm_bias_list,0) #[L,], 1.0 for places that need to be predicted\n",
        "        pssm_log_odds_ = np.concatenate(pssm_log_odds_list,0) #[L,], 1.0 for places that need to be predicted\n",
        "\n",
        "        bias_by_res_ = np.concatenate(bias_by_res_list, 0)  #[L,21], 0.0 for places where AA frequencies don't need to be tweaked\n",
        "\n",
        "        # Interestingly, all the chains are padded to the same length\n",
        "        # this has to be done most probably because the same layers are applied to all chains\n",
        "        # but for single chain or homomer cases, this should not be an issue\n",
        "        # need to be sure later why this is done\n",
        "        # does not significant when it comes to single chain energy-based usecase\n",
        "        # PADDING START\n",
        "        l = len(all_sequence)\n",
        "        x_pad = np.pad(x, [[0,L_max-l], [0,0], [0,0]], 'constant', constant_values=(np.nan, ))\n",
        "        X[i,:,:,:] = x_pad\n",
        "\n",
        "        m_pad = np.pad(m, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
        "        m_pos_pad = np.pad(m_pos, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
        "        omit_AA_mask_pad = np.pad(np.concatenate(omit_AA_mask_list,0), [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
        "        chain_M[i,:] = m_pad\n",
        "        chain_M_pos[i,:] = m_pos_pad\n",
        "        omit_AA_mask[i,] = omit_AA_mask_pad\n",
        "\n",
        "        chain_encoding_pad = np.pad(chain_encoding, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
        "        chain_encoding_all[i,:] = chain_encoding_pad\n",
        "\n",
        "        pssm_coef_pad = np.pad(pssm_coef_, [[0,L_max-l]], 'constant', constant_values=(0.0, ))\n",
        "        pssm_bias_pad = np.pad(pssm_bias_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
        "        pssm_log_odds_pad = np.pad(pssm_log_odds_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
        "\n",
        "        pssm_coef_all[i,:] = pssm_coef_pad\n",
        "        pssm_bias_all[i,:] = pssm_bias_pad\n",
        "        pssm_log_odds_all[i,:] = pssm_log_odds_pad\n",
        "\n",
        "        bias_by_res_pad = np.pad(bias_by_res_, [[0,L_max-l], [0,0]], 'constant', constant_values=(0.0, ))\n",
        "        bias_by_res_all[i,:] = bias_by_res_pad\n",
        "        # PADDING END\n",
        "\n",
        "        # Convert to labels\n",
        "        indices = np.asarray([alphabet.index(a) for a in all_sequence], dtype=np.int32)\n",
        "        S[i, :l] = indices\n",
        "        letter_list_list.append(letter_list)\n",
        "        visible_list_list.append(visible_list)\n",
        "        masked_list_list.append(masked_list)\n",
        "        masked_chain_length_list_list.append(masked_chain_length_list)\n",
        "\n",
        "\n",
        "    isnan = np.isnan(X)\n",
        "    mask = np.isfinite(np.sum(X,(2,3))).astype(np.float32)\n",
        "    X[isnan] = 0.\n",
        "\n",
        "    # Conversion\n",
        "    pssm_coef_all = torch.from_numpy(pssm_coef_all).to(dtype=torch.float32, device=device)\n",
        "    pssm_bias_all = torch.from_numpy(pssm_bias_all).to(dtype=torch.float32, device=device)\n",
        "    pssm_log_odds_all = torch.from_numpy(pssm_log_odds_all).to(dtype=torch.float32, device=device)\n",
        "\n",
        "    tied_beta = torch.from_numpy(tied_beta).to(dtype=torch.float32, device=device)\n",
        "\n",
        "    jumps = ((residue_idx[:,1:]-residue_idx[:,:-1])==1).astype(np.float32)\n",
        "    bias_by_res_all = torch.from_numpy(bias_by_res_all).to(dtype=torch.float32, device=device)\n",
        "    phi_mask = np.pad(jumps, [[0,0],[1,0]])\n",
        "    psi_mask = np.pad(jumps, [[0,0],[0,1]])\n",
        "    omega_mask = np.pad(jumps, [[0,0],[0,1]])\n",
        "    dihedral_mask = np.concatenate([phi_mask[:,:,None], psi_mask[:,:,None], omega_mask[:,:,None]], -1) #[B,L,3]\n",
        "    dihedral_mask = torch.from_numpy(dihedral_mask).to(dtype=torch.float32, device=device)\n",
        "    residue_idx = torch.from_numpy(residue_idx).to(dtype=torch.long,device=device)\n",
        "    S = torch.from_numpy(S).to(dtype=torch.long,device=device)\n",
        "    X = torch.from_numpy(X).to(dtype=torch.float32, device=device)\n",
        "    mask = torch.from_numpy(mask).to(dtype=torch.float32, device=device)\n",
        "    chain_M = torch.from_numpy(chain_M).to(dtype=torch.float32, device=device)\n",
        "    chain_M_pos = torch.from_numpy(chain_M_pos).to(dtype=torch.float32, device=device)\n",
        "    omit_AA_mask = torch.from_numpy(omit_AA_mask).to(dtype=torch.float32, device=device)\n",
        "    chain_encoding_all = torch.from_numpy(chain_encoding_all).to(dtype=torch.long, device=device)\n",
        "    # in general, in this return statement, *_list_list has the list inside list format because the outer list corresponds to \"batch_clones\", \n",
        "    # whereas the inner list corresponds to \"chains\" for each of the elements of \"batch_clones\"\n",
        "    # \"masked_list_list\" contains names of the designable chains (which is my target for single chain energy), whereas \"visible_list_list\" \n",
        "    # contains names of the fixed chains (which should be empty for my single chain energy)\n",
        "    # for my single chain energy case, \"letter_list_list\" should be equal to \"masked_list_list\", and three lists should have one list for now\n",
        "    # \"chain_encoding_all\" should also contain chain-index related to the only single chain which should be 0 (all 0s)\n",
        "    # the last lists starting from \"tied_pos_list_of_lists_list\" to the end should be irrelevant for my single chain energy case\n",
        "    # but still it would be good to check the values of these irrelevant lists, and get an idea if everything makes sense or not\n",
        "    # \"chain_M_pos\" contains values from \"fixed_position_mask\" through \"m_pos\", which should get populated with 0.0 for fixed positions\n",
        "    # and 1.0 for designable positions, which can be controlled through the , which\n",
        "    # is controlled by \"fixed_position_dict\" input to this function from the running script\n",
        "    # \"chain_M\" is formed from \"m_pad\" which comes from \"m\" which comes from chain_mask = np.ones(chain_length) #1.0 for masked\n",
        "    # so, for my single chain energy usecase, \"chain_M\" should be all 1.0s with the same length as chain_M_pos\n",
        "    # I do not think \"X\", \"S\", and \"mask\" need to be manipulated for now \n",
        "    return X, S, mask, lengths, chain_M, chain_encoding_all, letter_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef_all, pssm_bias_all, pssm_log_odds_all, bias_by_res_all, tied_beta\n",
        "\n",
        "\n",
        "# No need to dig into this loss function for now\n",
        "def loss_nll(S, log_probs, mask):\n",
        "    \"\"\" Negative log probabilities \"\"\"\n",
        "    criterion = torch.nn.NLLLoss(reduction='none')\n",
        "    loss = criterion(\n",
        "        log_probs.contiguous().view(-1, log_probs.size(-1)), S.contiguous().view(-1)\n",
        "    ).view(S.size())\n",
        "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
        "    return loss, loss_av\n",
        "\n",
        "# No need to dig into this label smoothing stuff for now\n",
        "def loss_smoothed(S, log_probs, mask, weight=0.1):\n",
        "    \"\"\" Negative log probabilities \"\"\"\n",
        "    S_onehot = torch.nn.functional.one_hot(S, 21).float()\n",
        "\n",
        "    # Label smoothing\n",
        "    S_onehot = S_onehot + weight / float(S_onehot.size(-1))\n",
        "    S_onehot = S_onehot / S_onehot.sum(-1, keepdim=True)\n",
        "\n",
        "    loss = -(S_onehot * log_probs).sum(-1)\n",
        "    loss_av = torch.sum(loss * mask) / torch.sum(mask)\n",
        "    return loss, loss_av\n",
        "\n",
        "# Objects of this class can be indexed since dunder methods __len()__ and __getitem()__ have been implemented, which \n",
        "# indexes a list that has been declared as an instance variable in the constructor,\n",
        "# and each element of that underlying list is a dictionary containing information regarding a specific sequence\n",
        "class StructureDataset():\n",
        "    def __init__(self, jsonl_file, verbose=True, truncate=None, max_length=100,\n",
        "        alphabet='ACDEFGHIKLMNPQRSTVWYX-'):\n",
        "        alphabet_set = set([a for a in alphabet])\n",
        "        discard_count = {\n",
        "            'bad_chars': 0,\n",
        "            'too_long': 0,\n",
        "            'bad_seq_length': 0\n",
        "        }\n",
        "\n",
        "        with open(jsonl_file) as f:\n",
        "            self.data = []\n",
        "\n",
        "            lines = f.readlines()\n",
        "            start = time.time()\n",
        "            for i, line in enumerate(lines):\n",
        "                entry = json.loads(line)\n",
        "                seq = entry['seq'] \n",
        "                name = entry['name']\n",
        "\n",
        "                # Convert raw coords to np arrays\n",
        "                #for key, val in entry['coords'].items():\n",
        "                #    entry['coords'][key] = np.asarray(val)\n",
        "\n",
        "                # Check if in alphabet\n",
        "                bad_chars = set([s for s in seq]).difference(alphabet_set)\n",
        "                if len(bad_chars) == 0:\n",
        "                    if len(entry['seq']) <= max_length:\n",
        "                        if True:\n",
        "                            self.data.append(entry)\n",
        "                        else:\n",
        "                            discard_count['bad_seq_length'] += 1\n",
        "                    else:\n",
        "                        discard_count['too_long'] += 1\n",
        "                else:\n",
        "                    print(name, bad_chars, entry['seq'])\n",
        "                    discard_count['bad_chars'] += 1\n",
        "\n",
        "                # Truncate early\n",
        "                if truncate is not None and len(self.data) == truncate:\n",
        "                    return\n",
        "\n",
        "                if verbose and (i + 1) % 1000 == 0:\n",
        "                    elapsed = time.time() - start\n",
        "                    print('{} entries ({} loaded) in {:.1f} s'.format(len(self.data), i+1, elapsed))\n",
        "\n",
        "            print('discarded', discard_count)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "    \n",
        "\n",
        "# Objects of this class can be indexed since dunder methods __len()__ and __getitem()__ have been implemented, which \n",
        "# indexes a list that has been declared as an instance variable in the constructor,\n",
        "# and each element of that underlying list is a dictionary containing information regarding a specific structure,\n",
        "# seems like a structure-specific version of the above method which deals with sequences \n",
        "class StructureDatasetPDB():\n",
        "    def __init__(self, pdb_dict_list, verbose=True, truncate=None, max_length=100,\n",
        "        alphabet='ACDEFGHIKLMNPQRSTVWYX-'):\n",
        "        alphabet_set = set([a for a in alphabet])\n",
        "        discard_count = {\n",
        "            'bad_chars': 0,\n",
        "            'too_long': 0,\n",
        "            'bad_seq_length': 0\n",
        "        }\n",
        "\n",
        "        self.data = []\n",
        "\n",
        "        start = time.time()\n",
        "        # elements of pdb_dict_list are dictionaries containing information regarding a specific pdb file\n",
        "        for i, entry in enumerate(pdb_dict_list):\n",
        "            seq = entry['seq']\n",
        "            name = entry['name']\n",
        "\n",
        "            bad_chars = set([s for s in seq]).difference(alphabet_set)\n",
        "            if len(bad_chars) == 0:\n",
        "                if len(entry['seq']) <= max_length:\n",
        "                    self.data.append(entry)\n",
        "                else:\n",
        "                    discard_count['too_long'] += 1\n",
        "            else:\n",
        "                discard_count['bad_chars'] += 1\n",
        "\n",
        "            # Truncate early\n",
        "            if truncate is not None and len(self.data) == truncate:\n",
        "                return\n",
        "\n",
        "            if verbose and (i + 1) % 1000 == 0:\n",
        "                elapsed = time.time() - start\n",
        "\n",
        "            #print('Discarded', discard_count)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "    \n",
        "class StructureLoader():\n",
        "    def __init__(self, dataset, batch_size=100, shuffle=True,\n",
        "        collate_fn=lambda x:x, drop_last=False):\n",
        "        self.dataset = dataset\n",
        "        self.size = len(dataset)\n",
        "        self.lengths = [len(dataset[i]['seq']) for i in range(self.size)]\n",
        "        self.batch_size = batch_size\n",
        "        sorted_ix = np.argsort(self.lengths)\n",
        "\n",
        "        # Cluster into batches of similar sizes\n",
        "        clusters, batch = [], []\n",
        "        batch_max = 0\n",
        "        for ix in sorted_ix:\n",
        "            size = self.lengths[ix]\n",
        "            if size * (len(batch) + 1) <= self.batch_size:\n",
        "                batch.append(ix)\n",
        "                batch_max = size\n",
        "            else:\n",
        "                clusters.append(batch)\n",
        "                batch, batch_max = [], 0\n",
        "        if len(batch) > 0:\n",
        "            clusters.append(batch)\n",
        "        self.clusters = clusters\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clusters)\n",
        "\n",
        "    def __iter__(self):\n",
        "        np.random.shuffle(self.clusters)\n",
        "        for b_idx in self.clusters:\n",
        "            batch = [self.dataset[i] for i in b_idx]\n",
        "            yield batch\n",
        "            \n",
        "            \n",
        "            \n",
        "# The following gather functions\n",
        "def gather_edges(edges, neighbor_idx):\n",
        "    # Features [B,N,N,C] at Neighbor indices [B,N,K] => Neighbor features [B,N,K,C]\n",
        "    neighbors = neighbor_idx.unsqueeze(-1).expand(-1, -1, -1, edges.size(-1))\n",
        "    edge_features = torch.gather(edges, 2, neighbors)\n",
        "    return edge_features\n",
        "\n",
        "def gather_nodes(nodes, neighbor_idx):\n",
        "    # Features [B,N,C] at Neighbor indices [B,N,K] => [B,N,K,C]\n",
        "    # Flatten and expand indices per batch [B,N,K] => [B,NK] => [B,NK,C]\n",
        "    neighbors_flat = neighbor_idx.view((neighbor_idx.shape[0], -1))\n",
        "    neighbors_flat = neighbors_flat.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
        "    # Gather and re-pack\n",
        "    neighbor_features = torch.gather(nodes, 1, neighbors_flat)\n",
        "    neighbor_features = neighbor_features.view(list(neighbor_idx.shape)[:3] + [-1])\n",
        "    return neighbor_features\n",
        "\n",
        "def gather_nodes_t(nodes, neighbor_idx):\n",
        "    # Features [B,N,C] at Neighbor index [B,K] => Neighbor features[B,K,C]\n",
        "    idx_flat = neighbor_idx.unsqueeze(-1).expand(-1, -1, nodes.size(2))\n",
        "    neighbor_features = torch.gather(nodes, 1, idx_flat)\n",
        "    return neighbor_features\n",
        "\n",
        "def cat_neighbors_nodes(h_nodes, h_neighbors, E_idx):\n",
        "    h_nodes = gather_nodes(h_nodes, E_idx)\n",
        "    h_nn = torch.cat([h_neighbors, h_nodes], -1)\n",
        "    return h_nn\n",
        "\n",
        "\n",
        "class EncLayer(nn.Module):\n",
        "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
        "        super(EncLayer, self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_in = num_in\n",
        "        self.scale = scale\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(num_hidden)\n",
        "        self.norm2 = nn.LayerNorm(num_hidden)\n",
        "        self.norm3 = nn.LayerNorm(num_hidden)\n",
        "\n",
        "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W11 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W12 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W13 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.act = torch.nn.GELU()\n",
        "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
        "\n",
        "    def forward(self, h_V, h_E, E_idx, mask_V=None, mask_attend=None):\n",
        "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
        "\n",
        "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
        "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
        "        if mask_attend is not None:\n",
        "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "        dh = torch.sum(h_message, -2) / self.scale\n",
        "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
        "\n",
        "        dh = self.dense(h_V)\n",
        "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "        if mask_V is not None:\n",
        "            mask_V = mask_V.unsqueeze(-1)\n",
        "            h_V = mask_V * h_V\n",
        "\n",
        "        h_EV = cat_neighbors_nodes(h_V, h_E, E_idx)\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_EV.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_EV], -1)\n",
        "        h_message = self.W13(self.act(self.W12(self.act(self.W11(h_EV)))))\n",
        "        h_E = self.norm3(h_E + self.dropout3(h_message))\n",
        "        return h_V, h_E\n",
        "\n",
        "\n",
        "class DecLayer(nn.Module):\n",
        "    def __init__(self, num_hidden, num_in, dropout=0.1, num_heads=None, scale=30):\n",
        "        super(DecLayer, self).__init__()\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_in = num_in\n",
        "        self.scale = scale\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.norm1 = nn.LayerNorm(num_hidden)\n",
        "        self.norm2 = nn.LayerNorm(num_hidden)\n",
        "\n",
        "        self.W1 = nn.Linear(num_hidden + num_in, num_hidden, bias=True)\n",
        "        self.W2 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.W3 = nn.Linear(num_hidden, num_hidden, bias=True)\n",
        "        self.act = torch.nn.GELU()\n",
        "        self.dense = PositionWiseFeedForward(num_hidden, num_hidden * 4)\n",
        "\n",
        "    def forward(self, h_V, h_E, mask_V=None, mask_attend=None):\n",
        "        \"\"\" Parallel computation of full transformer layer \"\"\"\n",
        "\n",
        "        # Concatenate h_V_i to h_E_ij\n",
        "        h_V_expand = h_V.unsqueeze(-2).expand(-1,-1,h_E.size(-2),-1)\n",
        "        h_EV = torch.cat([h_V_expand, h_E], -1)\n",
        "\n",
        "        # Maybe, length of the message vector can serve as attention\n",
        "        h_message = self.W3(self.act(self.W2(self.act(self.W1(h_EV)))))\n",
        "        # the mask attend here is most probably just for zeroing out the padded positions\n",
        "        # I do not think it will matter that much\n",
        "        if mask_attend is not None:\n",
        "            h_message = mask_attend.unsqueeze(-1) * h_message\n",
        "            # why divide by 30 when we are dealing with 48 neighbors in the current version of the model?\n",
        "        # Let me check the messages corresponding to \n",
        "        dh = torch.sum(h_message, -2) / self.scale\n",
        "\n",
        "        h_V = self.norm1(h_V + self.dropout1(dh))\n",
        "\n",
        "        # Position-wise feedforward\n",
        "        dh = self.dense(h_V)\n",
        "        h_V = self.norm2(h_V + self.dropout2(dh))\n",
        "\n",
        "        if mask_V is not None:\n",
        "            mask_V = mask_V.unsqueeze(-1)\n",
        "            h_V = mask_V * h_V\n",
        "\n",
        "        # \"h_message\" can be returned without dividing by \"self.scale\" also\n",
        "        return h_V, (h_message/self.scale) \n",
        "\n",
        "\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, num_hidden, num_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.W_in = nn.Linear(num_hidden, num_ff, bias=True)\n",
        "        self.W_out = nn.Linear(num_ff, num_hidden, bias=True)\n",
        "        self.act = torch.nn.GELU()\n",
        "    def forward(self, h_V):\n",
        "        h = self.act(self.W_in(h_V))\n",
        "        h = self.W_out(h)\n",
        "        return h\n",
        "\n",
        "class PositionalEncodings(nn.Module):\n",
        "    def __init__(self, num_embeddings, max_relative_feature=32):\n",
        "        super(PositionalEncodings, self).__init__()\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.max_relative_feature = max_relative_feature\n",
        "        self.linear = nn.Linear(2*max_relative_feature+1+1, num_embeddings)\n",
        "\n",
        "    def forward(self, offset, mask):\n",
        "        d = torch.clip(offset + self.max_relative_feature, 0, 2*self.max_relative_feature)*mask + (1-mask)*(2*self.max_relative_feature+1)\n",
        "        d_onehot = torch.nn.functional.one_hot(d, 2*self.max_relative_feature+1+1)\n",
        "        E = self.linear(d_onehot.float())\n",
        "        return E\n",
        "\n",
        "# Does not look like this function needs to be modified for now to use the model as sort of an energy function\n",
        "# The only thing that could do something is \"top_k\", which can be changed for considering more or less neighbors\n",
        "# for each of the nodes, but that too I think does not matter if the default value of top_k is updated by parameter passing\n",
        "# This function is called from the model itself with node_features=128, edge_features=128, and top_k=48\n",
        "# ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
        "class ProteinFeatures(nn.Module):\n",
        "    def __init__(self, edge_features, node_features, num_positional_embeddings=16,\n",
        "        num_rbf=16, top_k=30, augment_eps=0., num_chain_embeddings=16):\n",
        "        \"\"\" Extract protein features \"\"\"\n",
        "        super(ProteinFeatures, self).__init__()\n",
        "        self.edge_features = edge_features\n",
        "        self.node_features = node_features\n",
        "        self.top_k = top_k\n",
        "        self.augment_eps = augment_eps \n",
        "        self.num_rbf = num_rbf\n",
        "        self.num_positional_embeddings = num_positional_embeddings\n",
        "\n",
        "        self.embeddings = PositionalEncodings(num_positional_embeddings)\n",
        "        node_in, edge_in = 6, num_positional_embeddings + num_rbf*25\n",
        "        self.edge_embedding = nn.Linear(edge_in, edge_features, bias=False)\n",
        "        self.norm_edges = nn.LayerNorm(edge_features)\n",
        "\n",
        "    # the output of this function MUST be analyzed either directly or via some other function to \n",
        "    # understand how to get \"index/position\" of neighbors\n",
        "    def _dist(self, X, mask, eps=1E-6):\n",
        "        mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
        "        dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
        "        D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
        "        D_max, _ = torch.max(D, -1, keepdim=True)\n",
        "        D_adjust = D + (1. - mask_2D) * D_max\n",
        "        sampled_top_k = self.top_k\n",
        "        D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(self.top_k, X.shape[1]), dim=-1, largest=False)\n",
        "        return D_neighbors, E_idx\n",
        "\n",
        "    def _rbf(self, D):\n",
        "        device = D.device\n",
        "        D_min, D_max, D_count = 2., 22., self.num_rbf\n",
        "        D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n",
        "        D_mu = D_mu.view([1,1,1,-1])\n",
        "        D_sigma = (D_max - D_min) / D_count\n",
        "        D_expand = torch.unsqueeze(D, -1)\n",
        "        RBF = torch.exp(-((D_expand - D_mu) / D_sigma)**2)\n",
        "        return RBF\n",
        "\n",
        "    def _get_rbf(self, A, B, E_idx):\n",
        "        D_A_B = torch.sqrt(torch.sum((A[:,:,None,:] - B[:,None,:,:])**2,-1) + 1e-6) #[B, L, L]\n",
        "        D_A_B_neighbors = gather_edges(D_A_B[:,:,:,None], E_idx)[:,:,:,0] #[B,L,K]\n",
        "        RBF_A_B = self._rbf(D_A_B_neighbors)\n",
        "        return RBF_A_B\n",
        "\n",
        "    # this function will be called with the arguments as forward(), but will return information regarding \n",
        "    # the neighbors which I will figure out a way to parse\n",
        "    def return_neighbor_info(self, X, mask, residue_idx, chain_labels):\n",
        "        b = X[:,:,1,:] - X[:,:,0,:]\n",
        "        c = X[:,:,2,:] - X[:,:,1,:]\n",
        "        a = torch.cross(b, c, dim=-1)\n",
        "        Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
        "        Ca = X[:,:,1,:]\n",
        "        N = X[:,:,0,:]\n",
        "        C = X[:,:,2,:]\n",
        "        O = X[:,:,3,:]\n",
        " \n",
        "        D_neighbors, E_idx = self._dist(Ca, mask)\n",
        "\n",
        "\n",
        "    def forward(self, X, mask, residue_idx, chain_labels):\n",
        "        if self.augment_eps > 0:\n",
        "            X = X + self.augment_eps * torch.randn_like(X)\n",
        "        \n",
        "        b = X[:,:,1,:] - X[:,:,0,:]\n",
        "        c = X[:,:,2,:] - X[:,:,1,:]\n",
        "        a = torch.cross(b, c, dim=-1)\n",
        "        Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
        "        Ca = X[:,:,1,:]\n",
        "        N = X[:,:,0,:]\n",
        "        C = X[:,:,2,:]\n",
        "        O = X[:,:,3,:]\n",
        " \n",
        "        D_neighbors, E_idx = self._dist(Ca, mask)\n",
        "\n",
        "        RBF_all = []\n",
        "        RBF_all.append(self._rbf(D_neighbors)) #Ca-Ca\n",
        "        RBF_all.append(self._get_rbf(N, N, E_idx)) #N-N\n",
        "        RBF_all.append(self._get_rbf(C, C, E_idx)) #C-C\n",
        "        RBF_all.append(self._get_rbf(O, O, E_idx)) #O-O\n",
        "        RBF_all.append(self._get_rbf(Cb, Cb, E_idx)) #Cb-Cb\n",
        "        RBF_all.append(self._get_rbf(Ca, N, E_idx)) #Ca-N\n",
        "        RBF_all.append(self._get_rbf(Ca, C, E_idx)) #Ca-C\n",
        "        RBF_all.append(self._get_rbf(Ca, O, E_idx)) #Ca-O\n",
        "        RBF_all.append(self._get_rbf(Ca, Cb, E_idx)) #Ca-Cb\n",
        "        RBF_all.append(self._get_rbf(N, C, E_idx)) #N-C\n",
        "        RBF_all.append(self._get_rbf(N, O, E_idx)) #N-O\n",
        "        RBF_all.append(self._get_rbf(N, Cb, E_idx)) #N-Cb\n",
        "        RBF_all.append(self._get_rbf(Cb, C, E_idx)) #Cb-C\n",
        "        RBF_all.append(self._get_rbf(Cb, O, E_idx)) #Cb-O\n",
        "        RBF_all.append(self._get_rbf(O, C, E_idx)) #O-C\n",
        "        RBF_all.append(self._get_rbf(N, Ca, E_idx)) #N-Ca\n",
        "        RBF_all.append(self._get_rbf(C, Ca, E_idx)) #C-Ca\n",
        "        RBF_all.append(self._get_rbf(O, Ca, E_idx)) #O-Ca\n",
        "        RBF_all.append(self._get_rbf(Cb, Ca, E_idx)) #Cb-Ca\n",
        "        RBF_all.append(self._get_rbf(C, N, E_idx)) #C-N\n",
        "        RBF_all.append(self._get_rbf(O, N, E_idx)) #O-N\n",
        "        RBF_all.append(self._get_rbf(Cb, N, E_idx)) #Cb-N\n",
        "        RBF_all.append(self._get_rbf(C, Cb, E_idx)) #C-Cb\n",
        "        RBF_all.append(self._get_rbf(O, Cb, E_idx)) #O-Cb\n",
        "        RBF_all.append(self._get_rbf(C, O, E_idx)) #C-O\n",
        "        RBF_all = torch.cat(tuple(RBF_all), dim=-1)\n",
        "\n",
        "        offset = residue_idx[:,:,None]-residue_idx[:,None,:]\n",
        "        offset = gather_edges(offset[:,:,:,None], E_idx)[:,:,:,0] #[B, L, K]\n",
        "\n",
        "        d_chains = ((chain_labels[:, :, None] - chain_labels[:,None,:])==0).long() #find self vs non-self interaction\n",
        "        E_chains = gather_edges(d_chains[:,:,:,None], E_idx)[:,:,:,0]\n",
        "        E_positional = self.embeddings(offset.long(), E_chains)\n",
        "        E = torch.cat((E_positional, RBF_all), -1)\n",
        "        E = self.edge_embedding(E)\n",
        "        E = self.norm_edges(E)\n",
        "        return E, E_idx \n",
        "\n",
        "\n",
        "\n",
        "class ProteinMPNN(nn.Module):\n",
        "    # \"node_features\" and \"edge_features\" are actually dimensionality of these features (\"hidden_dim\" in the calling script)\n",
        "    # the value is 128 for the version that I am using\n",
        "    def __init__(self, num_letters, node_features, edge_features,\n",
        "        hidden_dim, num_encoder_layers=3, num_decoder_layers=3,\n",
        "        vocab=21, k_neighbors=64, augment_eps=0.05, dropout=0.1):\n",
        "        super(ProteinMPNN, self).__init__()\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.node_features = node_features\n",
        "        self.edge_features = edge_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Featurization layers\n",
        "        # The version that I am using considers 48 neighbors for each position\n",
        "        self.features = ProteinFeatures(node_features, edge_features, top_k=k_neighbors, augment_eps=augment_eps)\n",
        "\n",
        "        self.W_e = nn.Linear(edge_features, hidden_dim, bias=True)\n",
        "        # This W_s is for embedding the sequence\n",
        "        self.W_s = nn.Embedding(vocab, hidden_dim)\n",
        "\n",
        "        # Encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncLayer(hidden_dim, hidden_dim*2, dropout=dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "\n",
        "        # Decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecLayer(hidden_dim, hidden_dim*3, dropout=dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        self.W_out = nn.Linear(hidden_dim, num_letters, bias=True)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    # Creating my own versions of forward should be an easy way to get embeddings or attention weights from diffrerent layers of the model\n",
        "    # See here (https://discuss.pytorch.org/t/how-can-i-extract-intermediate-layer-output-from-loaded-cnn-model/77301) in the forums for adding forward\n",
        "    # hooks or manipulating the forward method\n",
        "    # but my easy solution would be to create different versions of the forward method with different namaes, and calling them explicitly\n",
        "    # \"chain_M\" and \"mask\" seem to be the things that I need to understand very well and play-around with \n",
        "    def forward(self, X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, use_input_decoding_order=False, decoding_order=None):\n",
        "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
        "        device=X.device\n",
        "        # Prepare node and edge embeddings\n",
        "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
        "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
        "        h_E = self.W_e(E)\n",
        "\n",
        "        # Encoder is unmasked self-attention\n",
        "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
        "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
        "        for layer in self.encoder_layers:\n",
        "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
        "\n",
        "        # Concatenate sequence embeddings for autoregressive decoder\n",
        "        # h_S denotes embedding of the sequence itself for use in decoder\n",
        "        h_S = self.W_s(S)\n",
        "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
        "\n",
        "        # Build encoder embeddings\n",
        "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
        "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
        "\n",
        "\n",
        "        chain_M = chain_M*mask #update chain_M to include missing regions\n",
        "        if not use_input_decoding_order:\n",
        "            decoding_order = torch.argsort((chain_M+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
        "        mask_size = E_idx.shape[1]\n",
        "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
        "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
        "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
        "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
        "        mask_bw = mask_1D * mask_attend\n",
        "        mask_fw = mask_1D * (1. - mask_attend)\n",
        "\n",
        "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
        "        for layer in self.decoder_layers:\n",
        "            # Masked positions attend to encoder information, unmasked see. \n",
        "            h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
        "            h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
        "            # only the last layer decoder-messages will be stored in \"decoder_messages\"\n",
        "            h_V, decoder_messages = layer(h_V, h_ESV, mask)\n",
        "\n",
        "        logits = self.W_out(h_V)\n",
        "        # The probabilities are passed through log() function so that the sequences can be ranked based by summing the respective values \n",
        "        # for each position instead of multiplication \n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        # messages from the last layer decoder will also be returned for extracting neighbor-attention approximation\\\n",
        "        # last layer embeddings can be extracted from the \"h_V\" tensor \n",
        "        return log_probs, decoder_messages, h_V\n",
        "\n",
        "\n",
        "\n",
        "    # Seems like this is the method which is used by the notebook for calculating probabilites and scoring\n",
        "    # Need to dig into it thoroughly\n",
        "    # \"chain_mask\" and \"residue_idx\" seem like the tensors of interest\n",
        "    def sample(self, X, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=None, temperature=1.0, omit_AAs_np=None, bias_AAs_np=None, chain_M_pos=None, omit_AA_mask=None, pssm_coef=None, pssm_bias=None, pssm_multi=None, pssm_log_odds_flag=None, pssm_log_odds_mask=None, pssm_bias_flag=None, bias_by_res=None):\n",
        "        device = X.device\n",
        "        # Prepare node and edge embeddings\n",
        "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
        "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=device)\n",
        "        h_E = self.W_e(E)\n",
        "\n",
        "        # Encoder is unmasked self-attention\n",
        "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
        "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
        "        for layer in self.encoder_layers:\n",
        "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
        "\n",
        "        # Decoder uses masked self-attention\n",
        "        chain_mask = chain_mask*chain_M_pos*mask #update chain_M to include missing regions\n",
        "        decoding_order = torch.argsort((chain_mask+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
        "        mask_size = E_idx.shape[1]\n",
        "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
        "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
        "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
        "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
        "        mask_bw = mask_1D * mask_attend\n",
        "        mask_fw = mask_1D * (1. - mask_attend)\n",
        "\n",
        "        N_batch, N_nodes = X.size(0), X.size(1)\n",
        "        log_probs = torch.zeros((N_batch, N_nodes, 21), device=device)\n",
        "        all_probs = torch.zeros((N_batch, N_nodes, 21), device=device, dtype=torch.float32)\n",
        "        h_S = torch.zeros_like(h_V, device=device)\n",
        "        S = torch.zeros((N_batch, N_nodes), dtype=torch.int64, device=device)\n",
        "        h_V_stack = [h_V] + [torch.zeros_like(h_V, device=device) for _ in range(len(self.decoder_layers))]\n",
        "        constant = torch.tensor(omit_AAs_np, device=device)\n",
        "        constant_bias = torch.tensor(bias_AAs_np, device=device)\n",
        "        #chain_mask_combined = chain_mask*chain_M_pos \n",
        "        omit_AA_mask_flag = omit_AA_mask != None\n",
        "\n",
        "\n",
        "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
        "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
        "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
        "        for t_ in range(N_nodes):\n",
        "            t = decoding_order[:,t_] #[B]\n",
        "            chain_mask_gathered = torch.gather(chain_mask, 1, t[:,None]) #[B]\n",
        "            bias_by_res_gathered = torch.gather(bias_by_res, 1, t[:,None,None].repeat(1,1,21))[:,0,:] #[B, 21]\n",
        "            if (chain_mask_gathered==0).all():\n",
        "                S_t = torch.gather(S_true, 1, t[:,None])\n",
        "            else:\n",
        "                # Hidden layers\n",
        "                E_idx_t = torch.gather(E_idx, 1, t[:,None,None].repeat(1,1,E_idx.shape[-1]))\n",
        "                h_E_t = torch.gather(h_E, 1, t[:,None,None,None].repeat(1,1,h_E.shape[-2], h_E.shape[-1]))\n",
        "                h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)\n",
        "                h_EXV_encoder_t = torch.gather(h_EXV_encoder_fw, 1, t[:,None,None,None].repeat(1,1,h_EXV_encoder_fw.shape[-2], h_EXV_encoder_fw.shape[-1]))\n",
        "                mask_t = torch.gather(mask, 1, t[:,None])\n",
        "                for l, layer in enumerate(self.decoder_layers):\n",
        "                    # Updated relational features for future states\n",
        "                    h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)\n",
        "                    h_V_t = torch.gather(h_V_stack[l], 1, t[:,None,None].repeat(1,1,h_V_stack[l].shape[-1]))\n",
        "                    h_ESV_t = torch.gather(mask_bw, 1, t[:,None,None,None].repeat(1,1,mask_bw.shape[-2], mask_bw.shape[-1])) * h_ESV_decoder_t + h_EXV_encoder_t\n",
        "                    h_V_stack[l+1].scatter_(1, t[:,None,None].repeat(1,1,h_V.shape[-1]), layer(h_V_t, h_ESV_t, mask_V=mask_t))\n",
        "                # Sampling step\n",
        "                h_V_t = torch.gather(h_V_stack[-1], 1, t[:,None,None].repeat(1,1,h_V_stack[-1].shape[-1]))[:,0]\n",
        "                logits = self.W_out(h_V_t) / temperature\n",
        "                probs = F.softmax(logits-constant[None,:]*1e8+constant_bias[None,:]/temperature+bias_by_res_gathered/temperature, dim=-1)\n",
        "                if pssm_bias_flag:\n",
        "                    pssm_coef_gathered = torch.gather(pssm_coef, 1, t[:,None])[:,0]\n",
        "                    pssm_bias_gathered = torch.gather(pssm_bias, 1, t[:,None,None].repeat(1,1,pssm_bias.shape[-1]))[:,0]\n",
        "                    probs = (1-pssm_multi*pssm_coef_gathered[:,None])*probs + pssm_multi*pssm_coef_gathered[:,None]*pssm_bias_gathered\n",
        "                if pssm_log_odds_flag:\n",
        "                    pssm_log_odds_mask_gathered = torch.gather(pssm_log_odds_mask, 1, t[:,None, None].repeat(1,1,pssm_log_odds_mask.shape[-1]))[:,0] #[B, 21]\n",
        "                    probs_masked = probs*pssm_log_odds_mask_gathered\n",
        "                    probs_masked += probs * 0.001\n",
        "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
        "                if omit_AA_mask_flag:\n",
        "                    omit_AA_mask_gathered = torch.gather(omit_AA_mask, 1, t[:,None, None].repeat(1,1,omit_AA_mask.shape[-1]))[:,0] #[B, 21]\n",
        "                    probs_masked = probs*(1.0-omit_AA_mask_gathered)\n",
        "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
        "                # Here is where sampling from the multinomial distribution is happening\n",
        "                # this will sample 1 element according to the given distribution, and return the index of that element [from 0 to 20]\n",
        "                S_t = torch.multinomial(probs, 1)\n",
        "                all_probs.scatter_(1, t[:,None,None].repeat(1,1,21), (chain_mask_gathered[:,:,None,]*probs[:,None,:]).float())\n",
        "            S_true_gathered = torch.gather(S_true, 1, t[:,None])\n",
        "            S_t = (S_t*chain_mask_gathered+S_true_gathered*(1.0-chain_mask_gathered)).long()\n",
        "            temp1 = self.W_s(S_t)\n",
        "            h_S.scatter_(1, t[:,None,None].repeat(1,1,temp1.shape[-1]), temp1)\n",
        "            S.scatter_(1, t[:,None], S_t)\n",
        "        output_dict = {\"S\": S, \"probs\": all_probs, \"decoding_order\": decoding_order}\n",
        "        return output_dict\n",
        "\n",
        "\n",
        "    def tied_sample(self, X, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=None, temperature=1.0, omit_AAs_np=None, bias_AAs_np=None, chain_M_pos=None, omit_AA_mask=None, pssm_coef=None, pssm_bias=None, pssm_multi=None, pssm_log_odds_flag=None, pssm_log_odds_mask=None, pssm_bias_flag=None, tied_pos=None, tied_beta=None, bias_by_res=None):\n",
        "        device = X.device\n",
        "        # Prepare node and edge embeddings\n",
        "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
        "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=device)\n",
        "        h_E = self.W_e(E)\n",
        "        # Encoder is unmasked self-attention\n",
        "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
        "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
        "        for layer in self.encoder_layers:\n",
        "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
        "\n",
        "        # Decoder uses masked self-attention\n",
        "        chain_mask = chain_mask*chain_M_pos*mask #update chain_M to include missing regions\n",
        "        decoding_order = torch.argsort((chain_mask+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
        "\n",
        "        new_decoding_order = []\n",
        "        for t_dec in list(decoding_order[0,].cpu().data.numpy()):\n",
        "            if t_dec not in list(itertools.chain(*new_decoding_order)):\n",
        "                list_a = [item for item in tied_pos if t_dec in item]\n",
        "                if list_a:\n",
        "                    new_decoding_order.append(list_a[0])\n",
        "                else:\n",
        "                    new_decoding_order.append([t_dec])\n",
        "        decoding_order = torch.tensor(list(itertools.chain(*new_decoding_order)), device=device)[None,].repeat(X.shape[0],1)\n",
        "\n",
        "        mask_size = E_idx.shape[1]\n",
        "        permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
        "        order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
        "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
        "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
        "        mask_bw = mask_1D * mask_attend\n",
        "        mask_fw = mask_1D * (1. - mask_attend)\n",
        "\n",
        "        N_batch, N_nodes = X.size(0), X.size(1)\n",
        "        log_probs = torch.zeros((N_batch, N_nodes, 21), device=device)\n",
        "        all_probs = torch.zeros((N_batch, N_nodes, 21), device=device, dtype=torch.float32)\n",
        "        h_S = torch.zeros_like(h_V, device=device)\n",
        "        S = torch.zeros((N_batch, N_nodes), dtype=torch.int64, device=device)\n",
        "        h_V_stack = [h_V] + [torch.zeros_like(h_V, device=device) for _ in range(len(self.decoder_layers))]\n",
        "        constant = torch.tensor(omit_AAs_np, device=device)\n",
        "        constant_bias = torch.tensor(bias_AAs_np, device=device)\n",
        "        omit_AA_mask_flag = omit_AA_mask != None\n",
        "\n",
        "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
        "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
        "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
        "        for t_list in new_decoding_order:\n",
        "            logits = 0.0\n",
        "            logit_list = []\n",
        "            done_flag = False\n",
        "            for t in t_list:\n",
        "                if (chain_mask[:,t]==0).all():\n",
        "                    S_t = S_true[:,t]\n",
        "                    for t in t_list:\n",
        "                        h_S[:,t,:] = self.W_s(S_t)\n",
        "                        S[:,t] = S_t\n",
        "                    done_flag = True\n",
        "                    break\n",
        "                else:\n",
        "                    E_idx_t = E_idx[:,t:t+1,:]\n",
        "                    h_E_t = h_E[:,t:t+1,:,:]\n",
        "                    h_ES_t = cat_neighbors_nodes(h_S, h_E_t, E_idx_t)\n",
        "                    h_EXV_encoder_t = h_EXV_encoder_fw[:,t:t+1,:,:]\n",
        "                    mask_t = mask[:,t:t+1]\n",
        "                    for l, layer in enumerate(self.decoder_layers):\n",
        "                        h_ESV_decoder_t = cat_neighbors_nodes(h_V_stack[l], h_ES_t, E_idx_t)\n",
        "                        h_V_t = h_V_stack[l][:,t:t+1,:]\n",
        "                        h_ESV_t = mask_bw[:,t:t+1,:,:] * h_ESV_decoder_t + h_EXV_encoder_t\n",
        "                        h_V_stack[l+1][:,t,:] = layer(h_V_t, h_ESV_t, mask_V=mask_t).squeeze(1)\n",
        "                    h_V_t = h_V_stack[-1][:,t,:]\n",
        "                    logit_list.append((self.W_out(h_V_t) / temperature)/len(t_list))\n",
        "                    logits += tied_beta[t]*(self.W_out(h_V_t) / temperature)/len(t_list)\n",
        "            if done_flag:\n",
        "                pass\n",
        "            else:\n",
        "                bias_by_res_gathered = bias_by_res[:,t,:] #[B, 21]\n",
        "                probs = F.softmax(logits-constant[None,:]*1e8+constant_bias[None,:]/temperature+bias_by_res_gathered/temperature, dim=-1)\n",
        "                if pssm_bias_flag:\n",
        "                    pssm_coef_gathered = pssm_coef[:,t]\n",
        "                    pssm_bias_gathered = pssm_bias[:,t]\n",
        "                    probs = (1-pssm_multi*pssm_coef_gathered[:,None])*probs + pssm_multi*pssm_coef_gathered[:,None]*pssm_bias_gathered\n",
        "                if pssm_log_odds_flag:\n",
        "                    pssm_log_odds_mask_gathered = pssm_log_odds_mask[:,t]\n",
        "                    probs_masked = probs*pssm_log_odds_mask_gathered\n",
        "                    probs_masked += probs * 0.001\n",
        "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
        "                if omit_AA_mask_flag:\n",
        "                    omit_AA_mask_gathered = omit_AA_mask[:,t]\n",
        "                    probs_masked = probs*(1.0-omit_AA_mask_gathered)\n",
        "                    probs = probs_masked/torch.sum(probs_masked, dim=-1, keepdim=True) #[B, 21]\n",
        "                S_t_repeat = torch.multinomial(probs, 1).squeeze(-1)\n",
        "                for t in t_list:\n",
        "                    h_S[:,t,:] = self.W_s(S_t_repeat)\n",
        "                    S[:,t] = S_t_repeat\n",
        "                    all_probs[:,t,:] = probs.float()\n",
        "        output_dict = {\"S\": S, \"probs\": all_probs, \"decoding_order\": decoding_order}\n",
        "        return output_dict\n",
        "\n",
        "\n",
        "    # I am not seeing an immediate use of this method when the model is called through notebook\n",
        "    # So, will skip further commenting and digging for now\n",
        "    # But, seems like an interesting way of interacting with the model in a specific way, so\n",
        "    # might get back to this later\n",
        "    def conditional_probs(self, X, S, mask, chain_M, residue_idx, chain_encoding_all, randn, backbone_only=False):\n",
        "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
        "        device=X.device\n",
        "        # Prepare node and edge embeddings\n",
        "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
        "        h_V_enc = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
        "        h_E = self.W_e(E)\n",
        "\n",
        "        # Encoder is unmasked self-attention\n",
        "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
        "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
        "        for layer in self.encoder_layers:\n",
        "            h_V_enc, h_E = layer(h_V_enc, h_E, E_idx, mask, mask_attend)\n",
        "\n",
        "        # Concatenate sequence embeddings for autoregressive decoder\n",
        "        h_S = self.W_s(S)\n",
        "        h_ES = cat_neighbors_nodes(h_S, h_E, E_idx)\n",
        "\n",
        "        # Build encoder embeddings\n",
        "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_S), h_E, E_idx)\n",
        "        h_EXV_encoder = cat_neighbors_nodes(h_V_enc, h_EX_encoder, E_idx)\n",
        "\n",
        "\n",
        "        chain_M = chain_M*mask #update chain_M to include missing regions\n",
        "  \n",
        "        chain_M_np = chain_M.cpu().numpy()\n",
        "        idx_to_loop = np.argwhere(chain_M_np[0,:]==1)[:,0]\n",
        "        log_conditional_probs = torch.zeros([X.shape[0], chain_M.shape[1], 21], device=device).float()\n",
        "\n",
        "        for idx in idx_to_loop:\n",
        "            h_V = torch.clone(h_V_enc)\n",
        "            order_mask = torch.zeros(chain_M.shape[1], device=device).float()\n",
        "            if backbone_only:\n",
        "                order_mask = torch.ones(chain_M.shape[1], device=device).float()\n",
        "                order_mask[idx] = 0.\n",
        "            else:\n",
        "                order_mask = torch.zeros(chain_M.shape[1], device=device).float()\n",
        "                order_mask[idx] = 1.\n",
        "            decoding_order = torch.argsort((order_mask[None,]+0.0001)*(torch.abs(randn))) #[numbers will be smaller for places where chain_M = 0.0 and higher for places where chain_M = 1.0]\n",
        "            mask_size = E_idx.shape[1]\n",
        "            permutation_matrix_reverse = torch.nn.functional.one_hot(decoding_order, num_classes=mask_size).float()\n",
        "            order_mask_backward = torch.einsum('ij, biq, bjp->bqp',(1-torch.triu(torch.ones(mask_size,mask_size, device=device))), permutation_matrix_reverse, permutation_matrix_reverse)\n",
        "            mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
        "            mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
        "            mask_bw = mask_1D * mask_attend\n",
        "            mask_fw = mask_1D * (1. - mask_attend)\n",
        "\n",
        "            h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
        "            for layer in self.decoder_layers:\n",
        "                # Masked positions attend to encoder information, unmasked see. \n",
        "                h_ESV = cat_neighbors_nodes(h_V, h_ES, E_idx)\n",
        "                h_ESV = mask_bw * h_ESV + h_EXV_encoder_fw\n",
        "                h_V = layer(h_V, h_ESV, mask)\n",
        "\n",
        "            logits = self.W_out(h_V)\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            log_conditional_probs[:,idx,:] = log_probs[:,idx,:]\n",
        "        return log_conditional_probs\n",
        "\n",
        "\n",
        "    # I am not seeing an immediate use of this method when the model is called through notebook\n",
        "    # So, will skip further commenting and digging for now\n",
        "    # But, seems like an interesting way of interacting with the model in a specific way, so\n",
        "    # might get back to this later\n",
        "    def unconditional_probs(self, X, mask, residue_idx, chain_encoding_all):\n",
        "        \"\"\" Graph-conditioned sequence model \"\"\"\n",
        "        device=X.device\n",
        "        # Prepare node and edge embeddings\n",
        "        E, E_idx = self.features(X, mask, residue_idx, chain_encoding_all)\n",
        "        h_V = torch.zeros((E.shape[0], E.shape[1], E.shape[-1]), device=E.device)\n",
        "        h_E = self.W_e(E)\n",
        "\n",
        "        # Encoder is unmasked self-attention\n",
        "        mask_attend = gather_nodes(mask.unsqueeze(-1),  E_idx).squeeze(-1)\n",
        "        mask_attend = mask.unsqueeze(-1) * mask_attend\n",
        "        for layer in self.encoder_layers:\n",
        "            h_V, h_E = layer(h_V, h_E, E_idx, mask, mask_attend)\n",
        "\n",
        "        # Build encoder embeddings\n",
        "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_V), h_E, E_idx)\n",
        "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
        "\n",
        "        order_mask_backward = torch.zeros([X.shape[0], X.shape[1], X.shape[1]], device=device)\n",
        "        mask_attend = torch.gather(order_mask_backward, 2, E_idx).unsqueeze(-1)\n",
        "        mask_1D = mask.view([mask.size(0), mask.size(1), 1, 1])\n",
        "        mask_bw = mask_1D * mask_attend\n",
        "        mask_fw = mask_1D * (1. - mask_attend)\n",
        "\n",
        "        h_EXV_encoder_fw = mask_fw * h_EXV_encoder\n",
        "        for layer in self.decoder_layers:\n",
        "            h_V = layer(h_V, h_EXV_encoder_fw, mask)\n",
        "\n",
        "        logits = self.W_out(h_V)\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "HjbVWJkg7zik"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 128\n",
        "num_layers = 3 \n",
        "# Seems like, backbone_noise is set to 0 at inference path which seems logical\n",
        "backbone_noise=0.00\n",
        "mpnn_model = ProteinMPNN(num_letters=21, node_features=hidden_dim, edge_features=hidden_dim, hidden_dim=hidden_dim, num_encoder_layers=num_layers, num_decoder_layers=num_layers, augment_eps=backbone_noise, k_neighbors=checkpoint['num_edges'])\n",
        "mpnn_model.to(device)\n",
        "mpnn_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "mpnn_model.eval()"
      ],
      "metadata": {
        "id": "QBgBJd3J0N_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(checkpoint['model_state_dict'].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pYLpMQS-ill",
        "outputId": "92380978-0646-43f8-93d4-012087e84414"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['features.embeddings.linear.weight', 'features.embeddings.linear.bias', 'features.edge_embedding.weight', 'features.norm_edges.weight', 'features.norm_edges.bias', 'W_e.weight', 'W_e.bias', 'W_s.weight', 'encoder_layers.0.norm1.weight', 'encoder_layers.0.norm1.bias', 'encoder_layers.0.norm2.weight', 'encoder_layers.0.norm2.bias', 'encoder_layers.0.norm3.weight', 'encoder_layers.0.norm3.bias', 'encoder_layers.0.W1.weight', 'encoder_layers.0.W1.bias', 'encoder_layers.0.W2.weight', 'encoder_layers.0.W2.bias', 'encoder_layers.0.W3.weight', 'encoder_layers.0.W3.bias', 'encoder_layers.0.W11.weight', 'encoder_layers.0.W11.bias', 'encoder_layers.0.W12.weight', 'encoder_layers.0.W12.bias', 'encoder_layers.0.W13.weight', 'encoder_layers.0.W13.bias', 'encoder_layers.0.dense.W_in.weight', 'encoder_layers.0.dense.W_in.bias', 'encoder_layers.0.dense.W_out.weight', 'encoder_layers.0.dense.W_out.bias', 'encoder_layers.1.norm1.weight', 'encoder_layers.1.norm1.bias', 'encoder_layers.1.norm2.weight', 'encoder_layers.1.norm2.bias', 'encoder_layers.1.norm3.weight', 'encoder_layers.1.norm3.bias', 'encoder_layers.1.W1.weight', 'encoder_layers.1.W1.bias', 'encoder_layers.1.W2.weight', 'encoder_layers.1.W2.bias', 'encoder_layers.1.W3.weight', 'encoder_layers.1.W3.bias', 'encoder_layers.1.W11.weight', 'encoder_layers.1.W11.bias', 'encoder_layers.1.W12.weight', 'encoder_layers.1.W12.bias', 'encoder_layers.1.W13.weight', 'encoder_layers.1.W13.bias', 'encoder_layers.1.dense.W_in.weight', 'encoder_layers.1.dense.W_in.bias', 'encoder_layers.1.dense.W_out.weight', 'encoder_layers.1.dense.W_out.bias', 'encoder_layers.2.norm1.weight', 'encoder_layers.2.norm1.bias', 'encoder_layers.2.norm2.weight', 'encoder_layers.2.norm2.bias', 'encoder_layers.2.norm3.weight', 'encoder_layers.2.norm3.bias', 'encoder_layers.2.W1.weight', 'encoder_layers.2.W1.bias', 'encoder_layers.2.W2.weight', 'encoder_layers.2.W2.bias', 'encoder_layers.2.W3.weight', 'encoder_layers.2.W3.bias', 'encoder_layers.2.W11.weight', 'encoder_layers.2.W11.bias', 'encoder_layers.2.W12.weight', 'encoder_layers.2.W12.bias', 'encoder_layers.2.W13.weight', 'encoder_layers.2.W13.bias', 'encoder_layers.2.dense.W_in.weight', 'encoder_layers.2.dense.W_in.bias', 'encoder_layers.2.dense.W_out.weight', 'encoder_layers.2.dense.W_out.bias', 'decoder_layers.0.norm1.weight', 'decoder_layers.0.norm1.bias', 'decoder_layers.0.norm2.weight', 'decoder_layers.0.norm2.bias', 'decoder_layers.0.W1.weight', 'decoder_layers.0.W1.bias', 'decoder_layers.0.W2.weight', 'decoder_layers.0.W2.bias', 'decoder_layers.0.W3.weight', 'decoder_layers.0.W3.bias', 'decoder_layers.0.dense.W_in.weight', 'decoder_layers.0.dense.W_in.bias', 'decoder_layers.0.dense.W_out.weight', 'decoder_layers.0.dense.W_out.bias', 'decoder_layers.1.norm1.weight', 'decoder_layers.1.norm1.bias', 'decoder_layers.1.norm2.weight', 'decoder_layers.1.norm2.bias', 'decoder_layers.1.W1.weight', 'decoder_layers.1.W1.bias', 'decoder_layers.1.W2.weight', 'decoder_layers.1.W2.bias', 'decoder_layers.1.W3.weight', 'decoder_layers.1.W3.bias', 'decoder_layers.1.dense.W_in.weight', 'decoder_layers.1.dense.W_in.bias', 'decoder_layers.1.dense.W_out.weight', 'decoder_layers.1.dense.W_out.bias', 'decoder_layers.2.norm1.weight', 'decoder_layers.2.norm1.bias', 'decoder_layers.2.norm2.weight', 'decoder_layers.2.norm2.bias', 'decoder_layers.2.W1.weight', 'decoder_layers.2.W1.bias', 'decoder_layers.2.W2.weight', 'decoder_layers.2.W2.bias', 'decoder_layers.2.W3.weight', 'decoder_layers.2.W3.bias', 'decoder_layers.2.dense.W_in.weight', 'decoder_layers.2.dense.W_in.bias', 'decoder_layers.2.dense.W_out.weight', 'decoder_layers.2.dense.W_out.bias', 'W_out.weight', 'W_out.bias'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse and create dictionaries for all the mutations in PremPS 2648\n",
        "# This dictionary will be a dictionary of dictionaries, where outer-dict keys will be pdbid+mutchain and inner-dict keys will be (wild+pos+mut) and ddg\n",
        "# the icodes can be brought to picture later\n",
        "# this \"two_level_dict\" is literally used everywhere throughout this code for storing all the numbers that are compared with each other under feature-specific keys\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/ACCRE_PyRun_Setup/Data_s669_with_predictions.csv\")\n",
        "\n",
        "\n",
        "pdbIdsChains = list(dataset[\"Protein\"])\n",
        "\n",
        "pdbIds = []\n",
        "mutChains = []\n",
        "\n",
        "for pdbIdChain in pdbIdsChains:\n",
        "    pdbIds.append(pdbIdChain[0:-1])\n",
        "    mutChains.append(pdbIdChain[-1])\n",
        "\n",
        "mutations = list(dataset[\"PDB_Mut\"])\n",
        "ddgs = list(dataset[\"DDG_checked_dir\"])\n",
        "\n",
        "two_level_dict = {}\n",
        "\n",
        "for pdbId, mutChain, mutation, ddg in tqdm(zip(pdbIds,mutChains,mutations,ddgs)):\n",
        "    pos = [int(s) for s in re.findall('-?\\d+',mutation)][0]\n",
        "    wild = mutation[0]\n",
        "    mut = mutation[len(mutation)-1]\n",
        "\n",
        "    pdbId = pdbId.lower()\n",
        "\n",
        "    inner_dict = {}\n",
        "    inner_dict[\"mut\"] = f\"{wild}{pos}{mut}\"\n",
        "    inner_dict[\"ddg\"] = float(ddg)\n",
        "    outer_key = f\"{pdbId}{mutChain}\"\n",
        "    if outer_key not in two_level_dict:\n",
        "        two_level_dict[f\"{pdbId}{mutChain}\"] = [inner_dict]\n",
        "    else:\n",
        "        two_level_dict[f\"{pdbId}{mutChain}\"].append(inner_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e2e7c57f453340baaa195441e2ce38aa",
            "f4ee451d7f76430fb16cdf5282198629",
            "2ccd6a5b15204e4e921158b691d66a49",
            "b9ce1ed852fe4f9799c4b3df0e2de121",
            "895adf35bd9540a8a11118e4fa85b36e",
            "b5b9553b37a24a75b28a5eeb1c25cdac",
            "c2ba6bde3068456ab5eb0ad721bf3261",
            "ea1768949328415ab787707e133c4b54",
            "0c1fafee76dd4d1a88e1f3e0957f3e6e",
            "8433f763095b40a3bf777826eb02d4ed",
            "4fe2f57b0bdc436c819b874906a95064"
          ]
        },
        "id": "vP_unq7_sXrn",
        "outputId": "27d9973c-e801-4ef7-811f-b06bd725d891"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2e7c57f453340baaa195441e2ce38aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a seqres to position mapping dictionary\n",
        "# This dictionary will be a dictionary of dictionaries, where outer-dict keys will be pdbid+mutchain and inner-dict key will be (wild+pos) and value of 0-indexed position\n",
        "# the icodes can be brought to picture later\n",
        "mapping_dict = {}\n",
        "pdbDirectory = \"/content/drive/MyDrive/ACCRE_PyRun_Setup/S_669_PDB_Files\"\n",
        "parser = PDBParser(QUIET=True)\n",
        "# some proteins need to be skipped for now due to ICODE related discrapency\n",
        "proteins_to_skip = []\n",
        "\n",
        "for filename in tqdm(os.listdir(pdbDirectory)):\n",
        "    filepath = os.path.join(pdbDirectory,filename)\n",
        "    structure = parser.get_structure(id=filename.split(\".\")[0],file=filepath)\n",
        "    model = structure[0]\n",
        "    inner_dict = {}\n",
        "    outer_key = filename.split(\".\")[0]\n",
        "    skip_flag = False\n",
        "    # single chain-assumption in action again\n",
        "    for chain in model:\n",
        "        for i,residue in enumerate(chain):\n",
        "            inner_key = f\"{three_to_one(residue.get_resname())}{residue.get_id()[1]}\"\n",
        "            if inner_key not in inner_dict:\n",
        "                inner_dict[inner_key] = i\n",
        "            else:\n",
        "                # For \"2immA:N31\" and \"1lveA:S27\", I have been fucked\n",
        "                # Need to think whether this will effect other positions or I can just avoid these two-protein related mutations for now?\n",
        "                # Let me just avoid these two proteins for now\n",
        "                print(\"YOU HAVE JUST BEEN FUCKED BY ICODE\")\n",
        "                print(f\"{outer_key}:{inner_key}\")\n",
        "                skip_flag = True\n",
        "    # The ICODE related problematic proteins will not be considered for now\n",
        "    if not skip_flag:\n",
        "        mapping_dict[outer_key] = inner_dict\n",
        "    else:\n",
        "        proteins_to_skip.append(outer_key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fa090cadb889484eb2f1b20727389fff",
            "4f318cf0d34643e7bb114058a93b4014",
            "9061c05f5463497c9212f9d26e75f2fa",
            "f00cd0764b2f44e280e4d50d0258601e",
            "e62490ffcdc94d5497761260cb6f7af7",
            "f558d33793cc40bba9e4d124d65a5a43",
            "69cfeb5faf1b4ec7a93b17e1e654b5db",
            "60dd91e47a254e6abf79d1e590418d2b",
            "559c046103f24a14893bfb163c0c9afa",
            "d99c0ff7ec0e4fa4b34096db3e733533",
            "13a3bd48f9d444c98d62f81d0ac6aecd"
          ]
        },
        "id": "vxARThyX3VYv",
        "outputId": "b13d8698-0788-49f4-886f-97668fb78a28"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/93 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa090cadb889484eb2f1b20727389fff"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# changing this \"parse_PDB_biounits()\" function locally for addressing the fucked up integer named chain problem  \n",
        "def parse_PDB_biounits(x, atoms=['N', 'CA', 'C'], chain=None):\n",
        "    '''\n",
        "    input:  x = PDB filename\n",
        "            atoms = atoms to extract (optional)\n",
        "    output: (length, atoms, coords=(x,y,z)), sequence\n",
        "    '''\n",
        "    alpha_1 = list(\"ARNDCQEGHILKMFPSTWYV-\")\n",
        "    states = len(alpha_1)\n",
        "    alpha_3 = ['ALA', 'ARG', 'ASN', 'ASP', 'CYS', 'GLN', 'GLU', 'GLY', 'HIS', 'ILE',\n",
        "               'LEU', 'LYS', 'MET', 'PHE', 'PRO', 'SER', 'THR', 'TRP', 'TYR', 'VAL', 'GAP']\n",
        "\n",
        "    # The following dictionaries are mapping from one-letter to 0-20 index,\n",
        "    # three-letter to 0-20 index,\n",
        "    # 0-20 index to one-letter,\n",
        "    # one-letter to three-letter, and vice-versa\n",
        "    aa_1_N = {a: n for n, a in enumerate(alpha_1)}\n",
        "    aa_3_N = {a: n for n, a in enumerate(alpha_3)}\n",
        "    aa_N_1 = {n: a for n, a in enumerate(alpha_1)}\n",
        "    aa_1_3 = {a: b for a, b in zip(alpha_1, alpha_3)}\n",
        "    aa_3_1 = {b: a for a, b in zip(alpha_1, alpha_3)}\n",
        "\n",
        "    def AA_to_N(x):\n",
        "        # [\"ARND\"] -> [[0,1,2,3]]\n",
        "        x = np.array(x);\n",
        "        if x.ndim == 0: x = x[None]\n",
        "        return [[aa_1_N.get(a, states - 1) for a in y] for y in x]\n",
        "\n",
        "    def N_to_AA(x):\n",
        "        # [[0,1,2,3]] -> [\"ARND\"]\n",
        "        x = np.array(x);\n",
        "        if x.ndim == 1: x = x[None]\n",
        "        return [\"\".join([aa_N_1.get(a, \"-\") for a in y]) for y in x]\n",
        "\n",
        "    xyz, seq, min_resn, max_resn = {}, {}, 1e6, -1e6\n",
        "    for line in open(x, \"rb\"):\n",
        "        line = line.decode(\"utf-8\", \"ignore\").rstrip()\n",
        "\n",
        "        if line[:6] == \"HETATM\" and line[17:17 + 3] == \"MSE\":\n",
        "            line = line.replace(\"HETATM\", \"ATOM  \")\n",
        "            line = line.replace(\"MSE\", \"MET\")\n",
        "\n",
        "        if line[:4] == \"ATOM\":\n",
        "            ch = line[21:22]\n",
        "            # If the input chain is not in the PDB file, which can be the case if the target chains are named differently in the runner script,\n",
        "            # this line will cause the output to have literally no information, this is the case for integer named chains\n",
        "            # that does not mean that this line is not doing its job correctly, this is just a constraint that input chain names and\n",
        "            # chain names in the PDB file have to be congruent\n",
        "            # If \"ch\" is an integer, map it to alphabet, because input \"chain\" has been converted to alphabet\n",
        "            # In rare cases, some PDB files number chains with 1,2,3 instead of A,B,C\n",
        "            # This \"loc_dict\" dictionary contains integer to alphabet mapping for weird as fuck integer chain names\n",
        "            # This conversion will be done only when  chain name is actually an integer\n",
        "            if ord(ch) >= 49 and ord(ch) <= 57:\n",
        "                loc_dict = {(idx+1):ch for idx,ch in enumerate(ascii_uppercase)}\n",
        "                ch =  str(loc_dict[int(ch)])\n",
        "            if ch == chain or chain is None:\n",
        "                atom = line[12:12 + 4].strip()\n",
        "                resi = line[17:17 + 3]\n",
        "                resn = line[22:22 + 5].strip()\n",
        "                x, y, z = [float(line[i:(i + 8)]) for i in [30, 38, 46]]\n",
        "\n",
        "                if resn[-1].isalpha():\n",
        "                    resa, resn = resn[-1], int(resn[:-1]) - 1\n",
        "                else:\n",
        "                    resa, resn = \"\", int(resn) - 1\n",
        "                #         resn = int(resn)\n",
        "                if resn < min_resn:\n",
        "                    min_resn = resn\n",
        "                if resn > max_resn:\n",
        "                    max_resn = resn\n",
        "                if resn not in xyz:\n",
        "                    xyz[resn] = {}\n",
        "                if resa not in xyz[resn]:\n",
        "                    xyz[resn][resa] = {}\n",
        "                if resn not in seq:\n",
        "                    seq[resn] = {}\n",
        "                if resa not in seq[resn]:\n",
        "                    seq[resn][resa] = resi\n",
        "\n",
        "                if atom not in xyz[resn][resa]:\n",
        "                    xyz[resn][resa][atom] = np.array([x, y, z])\n",
        "\n",
        "    # convert to numpy arrays, fill in missing values\n",
        "    seq_, xyz_ = [], []\n",
        "    try:\n",
        "        for resn in range(min_resn, max_resn + 1):\n",
        "            if resn in seq:\n",
        "                for k in sorted(seq[resn]): seq_.append(aa_3_N.get(seq[resn][k], 20))\n",
        "            else:\n",
        "                seq_.append(20)\n",
        "            if resn in xyz:\n",
        "                for k in sorted(xyz[resn]):\n",
        "                    for atom in atoms:\n",
        "                        if atom in xyz[resn][k]:\n",
        "                            xyz_.append(xyz[resn][k][atom])\n",
        "                        else:\n",
        "                            xyz_.append(np.full(3, np.nan))\n",
        "            else:\n",
        "                for atom in atoms: xyz_.append(np.full(3, np.nan))\n",
        "        return np.array(xyz_).reshape(-1, len(atoms), 3), N_to_AA(np.array(seq_))\n",
        "    except TypeError:\n",
        "        return 'no_chain', 'no_chain'\n",
        "\n",
        "# Took this part out of \"utils.py\", and put here so that smalll changes can be made to address pesky issues like\n",
        "# integer named chain, and shit like those\n",
        "def parse_PDB(path_to_pdb, input_chain_list=None):\n",
        "    c=0\n",
        "    pdb_dict_list = []\n",
        "    init_alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J','K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T','U', 'V','W','X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g','h', 'i', 'j','k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't','u', 'v','w','x', 'y', 'z']\n",
        "    extra_alphabet = [str(item) for item in list(np.arange(300))]\n",
        "    chain_alphabet = init_alphabet + extra_alphabet\n",
        "     \n",
        "    if input_chain_list:\n",
        "        chain_alphabet = input_chain_list  \n",
        " \n",
        "\n",
        "    biounit_names = [path_to_pdb]\n",
        "    # Each of the biounits is a separate PDB file, so for running with a single PDB file like from colab, this loop will be executed only once\n",
        "    for biounit in biounit_names:\n",
        "        my_dict = {}\n",
        "        s = 0\n",
        "        concat_seq = ''\n",
        "        concat_N = []\n",
        "        concat_CA = []\n",
        "        concat_C = []\n",
        "        concat_O = []\n",
        "        concat_mask = []\n",
        "        coords_dict = {} \n",
        "        # This loop will be executed only once for single chain DDG type cases\n",
        "        for letter in chain_alphabet:\n",
        "            xyz, seq = parse_PDB_biounits(biounit, atoms=['N','CA','C','O'], chain=letter)\n",
        "            if type(xyz) != str:\n",
        "                concat_seq += seq[0]\n",
        "                my_dict['seq_chain_'+letter]=seq[0]\n",
        "                coords_dict_chain = {}\n",
        "                coords_dict_chain['N_chain_'+letter]=xyz[:,0,:].tolist()\n",
        "                coords_dict_chain['CA_chain_'+letter]=xyz[:,1,:].tolist()\n",
        "                coords_dict_chain['C_chain_'+letter]=xyz[:,2,:].tolist()\n",
        "                coords_dict_chain['O_chain_'+letter]=xyz[:,3,:].tolist()\n",
        "                my_dict['coords_chain_'+letter]=coords_dict_chain\n",
        "                s += 1\n",
        "        fi = biounit.rfind(\"/\")\n",
        "        my_dict['name']=biounit[(fi+1):-4]\n",
        "        my_dict['num_of_chains'] = s\n",
        "        my_dict['seq'] = concat_seq\n",
        "        if s <= len(chain_alphabet):\n",
        "            pdb_dict_list.append(my_dict)\n",
        "            c+=1\n",
        "    return pdb_dict_list"
      ],
      "metadata": {
        "id": "UzBk27pmlfh7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distance_func_local(X, mask, eps=1E-6):\n",
        "    mask_2D = torch.unsqueeze(mask,1) * torch.unsqueeze(mask,2)\n",
        "    dX = torch.unsqueeze(X,1) - torch.unsqueeze(X,2)\n",
        "    D = mask_2D * torch.sqrt(torch.sum(dX**2, 3) + eps)\n",
        "    D_max, _ = torch.max(D, -1, keepdim=True)\n",
        "    D_adjust = D + (1. - mask_2D) * D_max\n",
        "    top_k = checkpoint[\"num_edges\"]\n",
        "    sampled_top_k = top_k\n",
        "    D_neighbors, E_idx = torch.topk(D_adjust, np.minimum(top_k, X.shape[1]), dim=-1, largest=False)\n",
        "    return D_neighbors, E_idx\n",
        "\n",
        "def return_neighbor_info(X, mask):\n",
        "    b = X[:,:,1,:] - X[:,:,0,:]\n",
        "    c = X[:,:,2,:] - X[:,:,1,:]\n",
        "    a = torch.cross(b, c, dim=-1)\n",
        "    Cb = -0.58273431*a + 0.56802827*b - 0.54067466*c + X[:,:,1,:]\n",
        "    Ca = X[:,:,1,:]\n",
        "    N = X[:,:,0,:]\n",
        "    C = X[:,:,2,:]\n",
        "    O = X[:,:,3,:]\n",
        "\n",
        "    D_neighbors, E_idx = distance_func_local(Ca, mask)\n",
        "    # Got the indices of the neighbors, E_idx should be the 0-based indexing of the topK closest neighbors\n",
        "    # and D_neighbors should be the distances of those neighbors\n",
        "    return D_neighbors, E_idx"
      ],
      "metadata": {
        "id": "l6pA80oESa7Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "from scipy.special import kl_div"
      ],
      "metadata": {
        "id": "5mFg99eN_UqM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read in the PDB files from the directory where the S_2648 PDB Files are stored, and set-them up one by one for featuirization, and passing through the model\n",
        "pdbDirectory = \"/content/drive/MyDrive/ACCRE_PyRun_Setup/S_669_PDB_Files\"\n",
        "parser = PDBParser(QUIET=True)\n",
        "\n",
        "pdbId_info = []\n",
        "pos_info = []\n",
        "neighbor_pos = []\n",
        "neighbor_attention = []\n",
        "neighbor_attention_softmaxed = []\n",
        "neighbor_distances_tracking = []\n",
        "\n",
        "import time\n",
        "\n",
        "np.set_printoptions(suppress=True,precision=2)\n",
        "loc_proteins = []\n",
        "for i,filename in tqdm(enumerate(os.listdir(pdbDirectory))):\n",
        "    #ICODE related problematic proteins will be skipped from analysis for now\n",
        "    if (filename.split(\".\")[0] not in proteins_to_skip):\n",
        "        prot_start = time.time()\n",
        "        print(f\"Prot Processing:{i+1}\")\n",
        "        loc_proteins.append(filename.split(\".\")[0])\n",
        "        filepath = os.path.join(pdbDirectory,filename)\n",
        "        structure = parser.get_structure(id=filename.split(\".\")[0],file=filepath)\n",
        "        model = structure[0]\n",
        "        \n",
        "        # Since there is only one chain, and that same chain is both fixed designable for different residues, extracting that name, and putting them in pertinent lists\n",
        "        # taking chainname from filename since one of the files \"1rtpA.pdb\" has chain name with \"1\" instead of \"A\"\n",
        "        # fuck you motherfucking fucked up PDB file submitter. Have you shoved your head into your ass?\n",
        "        chain_name = (filename.split(\".\")[0])[-1]\n",
        "        fixed_chain_list = []\n",
        "        # the trick is to put the single chain as designable chain, and then create the \"fixed_positions_dict\" dictionary  \n",
        "        designed_chain_list = [chain_name]\n",
        "        chain_list = list(set(designed_chain_list + fixed_chain_list))\n",
        "\n",
        "        # Using the programs custome PDB parser for processing the PDB files\n",
        "        pdb_dict_list = parse_PDB(filepath, input_chain_list=chain_list)\n",
        "        # tacking max_length parameter value from the original colab notebook since I need to process all residues at the same time\n",
        "        # all the PDB files can technically be processed together and put inside the dataset_valid list-like object, but right now\n",
        "        # I am trying to keep everything consistent and simple\n",
        "        # Each element of dataset_valid is a dictionary \n",
        "        dataset_valid = StructureDatasetPDB(pdb_dict_list, truncate=None, max_length=20000)\n",
        "\n",
        "        # Simplying the sequence generation loop\n",
        "        protein = dataset_valid[0]\n",
        "\n",
        "        wildtype_seq = protein[f\"seq_chain_{designed_chain_list[0]}\"]\n",
        "\n",
        "        # If there are gaps in the wildtype_seq \"seq\", remove those positions from both the \"seq\", \"\" and ('coords_chain_{designed_chain_list[0]}'), \n",
        "        # and ('seq_chain_{designed_chain_list[0]}') of the \"protein\"\n",
        "        # print(protein.keys())\n",
        "        # protein is a dict with keys(['seq_chain_A', 'coords_chain_A', 'name', 'num_of_chains', 'seq'])\n",
        "        # \"seq_chain\" and \"seq_all\" are both strings of the same length where gapped positions need to be identified and removed\n",
        "        seq_chain = protein[f\"seq_chain_{designed_chain_list[0]}\"]\n",
        "        seq_all = protein[f\"seq\"]\n",
        "        # \"coordinates_chain\" is a dict with keys(['N_chain_A', 'CA_chain_A', 'C_chain_A', 'O_chain_A'])\n",
        "        coordinates_chain = protein[f\"coords_chain_{designed_chain_list[0]}\"]\n",
        "\n",
        "        \n",
        "        # The following four variables are lists of length equal to seq_chain and seq_all length\n",
        "        # Therefore, the gapped positions can be retrived from seq_chain and removed from everything accordingly\n",
        "        N_chain = coordinates_chain[f\"N_chain_{designed_chain_list[0]}\"]\n",
        "        CA_chain = coordinates_chain[f\"CA_chain_{designed_chain_list[0]}\"]\n",
        "        C_chain = coordinates_chain[f\"C_chain_{designed_chain_list[0]}\"]\n",
        "        O_chain = coordinates_chain[f\"O_chain_{designed_chain_list[0]}\"]\n",
        "\n",
        "        # delete everything related to gapped positions now\n",
        "        # at first, find out the positions that are gapped\n",
        "        # these gapped positions are absolutely messed up fucked up artifact of some kind of sophistification \n",
        "        # provided by proteinMPNN, FUCK YOU motherfucking oversmart CODERS\n",
        "        N_chain = [v for i,v in enumerate(N_chain) if seq_chain[i] != \"-\"]\n",
        "        CA_chain = [v for i,v in enumerate(CA_chain) if seq_chain[i] != \"-\"]\n",
        "        C_chain = [v for i,v in enumerate(C_chain) if seq_chain[i] != \"-\"]\n",
        "        O_chain = [v for i,v in enumerate(O_chain) if seq_chain[i] != \"-\"]\n",
        "        seq_all = [v for i,v in enumerate(seq_all) if seq_chain[i] != \"-\"]\n",
        "        seq_chain = [v for i,v in enumerate(seq_chain) if seq_chain[i] != \"-\"]\n",
        "\n",
        "        # Now, finally, pack everything back to the dictionary \"protein\"\n",
        "        protein[f\"seq_chain_{designed_chain_list[0]}\"] = seq_chain\n",
        "        protein[f\"seq\"] = seq_all\n",
        "        coordinates_chain[f\"N_chain_{designed_chain_list[0]}\"] = N_chain\n",
        "        coordinates_chain[f\"CA_chain_{designed_chain_list[0]}\"] = CA_chain\n",
        "        coordinates_chain[f\"C_chain_{designed_chain_list[0]}\"] = C_chain\n",
        "        coordinates_chain[f\"O_chain_{designed_chain_list[0]}\"] = O_chain\n",
        "        protein[f\"coords_chain_{designed_chain_list[0]}\"] = coordinates_chain\n",
        "\n",
        "        # At this point, probably need to put None values in a lot of parameters that are not relevant to my usecase, but need to be sent to featurizer before running model forward\n",
        "        # For now, I will not tie positions together\n",
        "        tied_positions_dict = None\n",
        "        pssm_dict = None\n",
        "        omit_AA_dict = None\n",
        "        bias_AA_dict = None\n",
        "        tied_positions_dict = None\n",
        "        bias_by_res_dict = None\n",
        "        alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
        "        bias_AAs_np = np.zeros(len(alphabet))\n",
        "\n",
        "        chain_id_dict = {}\n",
        "        chain_id_dict[pdb_dict_list[0]['name']]= (designed_chain_list, fixed_chain_list)\n",
        "\n",
        "        BATCH_COPIES = 1\n",
        "\n",
        "        batch_clones = [copy.deepcopy(protein) for i in range(BATCH_COPIES)]\n",
        "\n",
        "        # \"muts_for_prot\" is a list with information about all the mutations in \"protein\", whose sequence only version is \"wildtype_seq\" \n",
        "        muts_for_prot = two_level_dict[filename.split(\".\")[0]]\n",
        "        # \"cur_map_dict\" will give the 0-based sequence index for the mutations, which will be almost directly used for masking and then running the model\n",
        "        # 1-based indexing needed for the fixed position\n",
        "        cur_map_dict = mapping_dict[filename.split(\".\")[0]]\n",
        "\n",
        "        for mut_track,mut in enumerate(muts_for_prot):\n",
        "            print(f\"Processing_Mut:{mut_track+1}, From_Prot:{i+1}\")\n",
        "            wild_aa = mut[\"mut\"][0]\n",
        "            alternate_aa = mut[\"mut\"][-1]\n",
        "            # (+1) because we need to pass 1-based indexing to tied_featurize() method\n",
        "            seq_pos = cur_map_dict[mut[\"mut\"][0:-1]] + 1\n",
        "            # only need to mask the mutated position position in \"wildtype_seq\" for now\n",
        "            fixed_positions_dict = {}\n",
        "            fixed_positions_dict[protein[\"name\"]] = {}\n",
        "            f_list = []\n",
        "            for ind_fixed in range(0,len(seq_chain)):\n",
        "                if (ind_fixed + 1) not in [seq_pos]:\n",
        "                    f_list.append(ind_fixed + 1)\n",
        "            fixed_positions_dict[protein[\"name\"]][filename.split(\".\")[0][-1]] = f_list\n",
        "\n",
        "            # finally, had to take chain-name from filename instead of biopython parsing to get rid of chain-name with \"1\" instead of \"A\" in \"1rtpA.pdb\"\n",
        "            X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, \\\n",
        "            omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta  \\\n",
        "            = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
        "            randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "            log_probs, decoder_messages, node_embedding_info = mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)\n",
        "            # Adding the log_probs to the same inner dictionary where DDG values exist for easier comparison\n",
        "            mut[\"log_prob\"] = log_probs.cpu().data.numpy()\n",
        "            \n",
        "            # the top_k attention weights will be stored here for weighted sum later\n",
        "            # \"seq_pos\" is 1-based since \"fixed_positions_dict\" above needs to hold 1-based indices for the mutation,\n",
        "            # but accessing the tensors will require 0-based indexing\n",
        "            seq_index = seq_pos - 1\n",
        "            # \"dim = 1\", because decoder_messages[0,seq_index,:,:] should be (48,128), and we want to take norm of each of the 48 vectors across the last dimension,\n",
        "            # get 48 norm values, and fetch out the k highest values from there\n",
        "            message_norms = (torch.linalg.vector_norm(x=decoder_messages[0,seq_index,:,:],ord=2,dim=1))\n",
        "            local_distances, local_neighbors = return_neighbor_info(X, mask)\n",
        "            # \"top_k_attended_neighbor_indices\" is the indices from [0,47] corresponding to the highest attended neighbors\n",
        "            # top_(k-1) can technically be extracted from top_k, but currently just for simplicity and easier debugging, extracting everything separately\n",
        "            top_15_attention_vals, top_15_attended_neighbor_indices = torch.topk(message_norms,k=15)\n",
        "            top_10_attention_vals, top_10_attended_neighbor_indices = torch.topk(message_norms,k=10)\n",
        "            top_5_attention_vals, top_5_attended_neighbor_indices = torch.topk(message_norms,k=5)\n",
        "            # now, using \"local_neighbors\" to get the original indices of the neighbors corresponding to the top_k attended positions\n",
        "            # taking both top_5 and top_10 for now\n",
        "            top_15_attended_neighbor_indices = local_neighbors[0,seq_index,top_15_attended_neighbor_indices]\n",
        "            top_10_attended_neighbor_indices = local_neighbors[0,seq_index,top_10_attended_neighbor_indices]\n",
        "            top_5_attended_neighbor_indices = local_neighbors[0,seq_index,top_5_attended_neighbor_indices]\n",
        "            top_15_closest_neighbor_indices = local_neighbors[0,seq_index,1:16]\n",
        "            top_10_closest_neighbor_indices = local_neighbors[0,seq_index,1:11]\n",
        "            mut[\"top_15_attention_weights\"] = top_15_attention_vals.cpu().data.numpy()\n",
        "            mut[\"top_10_attention_weights\"] = top_10_attention_vals.cpu().data.numpy()\n",
        "            mut[\"top_5_attention_weights\"] = top_5_attention_vals.cpu().data.numpy()\n",
        "            # the neighbor indices corresponding to the top_k attention weights will be stored here for entropy calculation later\n",
        "            # 0-based indices of the neighbors will be saved so that corresponding log-probability vectors can be extracted readily from \"log_prob\" keyed value\n",
        "            mut[\"top_15_neighbor_indices\"] = top_15_attended_neighbor_indices.cpu().data.numpy()\n",
        "            mut[\"top_10_neighbor_indices\"] = top_10_attended_neighbor_indices.cpu().data.numpy()\n",
        "            mut[\"top_5_neighbor_indices\"] = top_5_attended_neighbor_indices.cpu().data.numpy()\n",
        "            mut[\"top_15_closest_neighbor_indices\"] = top_15_closest_neighbor_indices.cpu().data.numpy()\n",
        "            mut[\"top_10_closest_neighbor_indices\"] = top_10_closest_neighbor_indices.cpu().data.numpy()\n",
        "\n",
        "            # The lines below are mostly for printing purposes to do external analysis with PyMol,and ROSETTA with Cristina\n",
        "            loc_pos_scores = []\n",
        "            for enum_val,(neighbor_p, neighbor_s, neighbor_distance) in enumerate(zip(local_neighbors[0,seq_index,1:15].cpu().data.numpy(),\n",
        "                                                                 (torch.linalg.vector_norm(x=decoder_messages[0,seq_index,1:15,:],ord=2,dim=1)).cpu().data.numpy(),\n",
        "                  \n",
        "                                                               local_distances[0,seq_index,1:15].cpu().data.numpy())):\n",
        "                # skippoing the first neighbor since it is the mutated position itself\n",
        "                if enum_val == 0:\n",
        "                    continue\n",
        "                pdbId_info.append(filename)\n",
        "                pos_info.append(seq_index+1)\n",
        "                neighbor_pos.append(neighbor_p+1)\n",
        "                neighbor_attention.append(neighbor_s)\n",
        "                loc_pos_scores.append(neighbor_s)\n",
        "                neighbor_distances_tracking.append(neighbor_distance)\n",
        "            # since softmax has to be done over all the neighbors, taking the softmax, and later adding it to the data generation list after\n",
        "            # neighbor enumeration loop\n",
        "            loc_pos_scores = softmax(np.array(loc_pos_scores))\n",
        "            for s in loc_pos_scores:\n",
        "                neighbor_attention_softmaxed.append(s)\n",
        "\n",
        "            # take (\"neighbor_attention\"-weighted sum/average) of the entropies of the top 5 attended neighbors\n",
        "            # are the neighbors with highest message passing values always among the closest 10?\n",
        "            # point to be noted that the closest, therefore the first neighbor of every position is the neighbor itself\n",
        "            # check correlation between distance and attention values since a possible manual edge feature would be distance\n",
        "            # let us see if the model attends to distant neighbors more, or attention value is inversely proportional to distance?\n",
        "            # take L2-norms of the message vectors instead of attention\n",
        "            # take the softmax of L2-norms to approximate attention, although technically the positions are not constrained by each other, this can be considered a sigmoid attention\n",
        "            # where having neighbors with large messages will effect differently than having neighbors with small messages\n",
        "            # can this be correlated with position-entropy?\n",
        "            # the L2-norms should be able to approximate how much each of the neighbors are effecting the mutated position\n",
        "            # This information can be stored for checking the effect of center mutation on those positions afterwards\n",
        "            # Identify major interacting partners (neighbors that are important for center prediction, and also which take center into consideration for its own prediction)\n",
        "            # then check how much the major neighbor position deviates from wildtype due to the mutation\n",
        "            # another much more simples thing can be to check the deviation for top 10 neighbors\n",
        "            # this deviation can be calculated using the log(W) for the neighbor before center mutation, and log(W) for the neighbor after center mutation \n",
        "\n",
        "            # Now, take top_k most attended positions, make them designable, mutate center, and take change in -log(p) of the wildtype at each of the neighbor positions\n",
        "            # take weighted sum of these neighbor energy changes, see if there is any correlation\n",
        "            # next, go for \"strong\" neighbor positions (both way strong attention)\n",
        "            # Many of the tensors will take on new values after running the model again with different fixed positions\n",
        "            \n",
        "            # the \"fixed_positions_dict\" has to be repopulated now since neighbor positions will be masked one by one\n",
        "            # here, \"n_ind\" is the 0-based index corresponding to one of the \"top_k\" attended neighbors \n",
        "            # these lists will contain the log_probabilities for the top_k most attended neighbors serially\n",
        "            # before and after making the center mutation currently at hand, these probabilities will be used later for calculating\n",
        "            # attention_weighted change in neighbor_wildtype probability, attention_weighted_change in KL, and all those things \n",
        "            neighbor_w_log_probs = []\n",
        "            neighbor_m_log_probs = []\n",
        "            # the following array will contain the identities of the neighbors serially so that specific wildtype neighbor positions in the probability\n",
        "            # distribution can be extracted later\n",
        "            neighbor_aa_identities = []\n",
        "            # \"top5\" and \"top10\" should be extractable from \"top15\", since the neighbor\n",
        "            neighbor_w_message_vector_coming_from_center = []\n",
        "            neighbor_m_message_vector_coming_from_center = []\n",
        "            # I will pick out the neighbor embeddings, and put them inside the lists below\n",
        "            # But, do I need to access them one by one, or can I just take a slice out of the embedding tensor across the seq_pos dimension?\n",
        "            # what more information will I get if I fetch out the embeddings one by one?\n",
        "            # if I want to do a slicing, I might have to make all the neighbors designable at the same time, which will \n",
        "            # for one, I can take out the embeddings for each of the neighbors while they are designable, before and \n",
        "            # okay, lets say we will fetch the neighbors out, one by one; we can do that right inside the next loop\n",
        "            # what will be the significance of those neighbor embeddings, in that case?\n",
        "            # they will see everything around them, expcept the mutated position, will they change much due to that?\n",
        "            # neighbor embedding difference in that case might actually catch something about the new interactions that have\n",
        "            # been formed, interesting...very, very interesting\n",
        "            neighbor_w_embedding_info = []\n",
        "            neighbor_m_embedding_info = [] \n",
        "            # informations are stored serially in the lists\n",
        "            for n_ind in mut[\"top_15_neighbor_indices\"]:\n",
        "                neighbor_aa_identities.append(seq_chain[n_ind])\n",
        "                # Some sequence-input manipulation is done later in this loop, so the wildtype aa is placed in the position, so that\n",
        "                # previous iteration manipulations do not cause trouble in this iteration\n",
        "                alpha_tok = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "                aa_1_N = {a:n for n,a in enumerate(alpha_tok)}\n",
        "                aa_N_1 = {n:a for n,a in enumerate(alpha_tok)}\n",
        "\n",
        "                # adding (+1) to n_ind, since fixed positions are 1-indexed in the original implementation\n",
        "                n_pos = n_ind + 1  \n",
        "                fixed_positions_dict = {}\n",
        "                fixed_positions_dict[protein[\"name\"]] = {}\n",
        "                f_list = []\n",
        "                for ind_fixed in range(0,len(seq_chain)):\n",
        "                    # Fixing everything except the \"n_pos\" neighbor position\n",
        "                    if (ind_fixed + 1) not in [n_pos]:\n",
        "                        f_list.append(ind_fixed + 1)\n",
        "                fixed_positions_dict[protein[\"name\"]][filename.split(\".\")[0][-1]] = f_list\n",
        "\n",
        "                # Extracting \"n_ind\" 21-way log probabilities when the center is wildtype \n",
        "                X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, \\\n",
        "                omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta  \\\n",
        "                = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
        "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "\n",
        "                # we want to store the 128-length message vector coming from the center to the neighbor at \"n_ind\"\n",
        "                # for this, at first, we need to find which neighbor of \"n_ind\" is our center so that we can pull out the message vector coming from \n",
        "                # the center to the neighbor at \"n_ind\" \n",
        "                loc_local_distances, loc_local_neighbors = return_neighbor_info(X, mask)\n",
        "                # \"neighbor_neighbor_index\" is the index of the center with respect to the neighbor at \"n_ind\"\n",
        "                # if len(neighbor_neighbor_index) is 0, center is not among the spatially close 48 neighbors of the neighbor at \"n_ind\"\n",
        "                # in that case, message vector passed from center to that neighbor can be considered as a 128 length all 0 vector for now\n",
        "                # for now, it is important to note that if len(neighbor_neighbor_index) is not zero, then the index can be retrived by neighbor_neighbor_index[0][0]  \n",
        "                neighbor_neighbor_index = (loc_local_neighbors[0,n_ind,:] == seq_index).nonzero(as_tuple=False)\n",
        "                neighbor_neighbor_index =  neighbor_neighbor_index[0][0] if (len(neighbor_neighbor_index) == 1) else torch.tensor(-1,device=neighbor_neighbor_index.device)\n",
        "\n",
        "                # calling \"mpnn_model\" \"forward\" function three times below for getting the three tensors is definitely not efficient,\n",
        "                # but, doing it this way for now, since I am getting CUDA out-of-memory errors due to some unsolved (for now) reason   \n",
        "                n_log_probs  = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[0][0,n_ind,:].cpu().data.numpy()\n",
        "                decoder_message = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[1][0,n_ind,neighbor_neighbor_index,:].cpu().data.numpy()\n",
        "                node_embedding_info = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[2][0,n_ind,:].cpu().data.numpy()\n",
        "\n",
        "                neighbor_w_log_probs.append(n_log_probs)\n",
        "                neighbor_w_embedding_info.append(node_embedding_info)\n",
        "\n",
        "                if neighbor_neighbor_index >= 0:\n",
        "                    neighbor_w_message_vector_coming_from_center.append(decoder_message)\n",
        "                else:\n",
        "                    # Adding all-0 vector of length 128 if the center is not among the closest 48 neighbors of center\n",
        "                    neighbor_w_message_vector_coming_from_center.append(np.zeros(128)) \n",
        "                 \n",
        "                # Now, make mutation, and process the corresponding probabilities\n",
        "                X, S, mask, lengths, chain_M, chain_encoding_all, chain_list_list, visible_list_list, masked_list_list, masked_chain_length_list_list, chain_M_pos, \\\n",
        "                omit_AA_mask, residue_idx, dihedral_mask, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds_all, bias_by_res_all, tied_beta  \\\n",
        "                = tied_featurize(batch_clones, device, chain_id_dict, fixed_positions_dict, omit_AA_dict, tied_positions_dict, pssm_dict, bias_by_res_dict)\n",
        "                randn_1 = torch.randn(chain_M.shape, device=X.device)\n",
        "                # seems like passing mutant sequence through the model will be a bit more difficult that expected since PDB file is read in by the underlying parser\n",
        "                # How, do I only change the amino acids identity in the sequence, but keep the PDB backbone and everything same?\n",
        "                # At first, just try to manipulate the input \"S\"\n",
        "                S[0,seq_index] = aa_1_N[alternate_aa]\n",
        "                n_log_probs  = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[0][0,n_ind,:].cpu().data.numpy()\n",
        "                decoder_message = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[1][0,n_ind,neighbor_neighbor_index,:].cpu().data.numpy()\n",
        "                node_embedding_info = \\\n",
        "                    mpnn_model(X, S, mask, chain_M*chain_M_pos, residue_idx, chain_encoding_all, randn_1)[2][0,n_ind,:].cpu().data.numpy()\n",
        "\n",
        "                neighbor_m_log_probs.append(n_log_probs)\n",
        "                neighbor_m_embedding_info.append(node_embedding_info)\n",
        "\n",
        "                if neighbor_neighbor_index >= 0:\n",
        "                    neighbor_m_message_vector_coming_from_center.append(decoder_message)\n",
        "                else:\n",
        "                    # Adding all-0 vector of length 128 if the center is not among the closest 48 neighbors of center\n",
        "                    neighbor_m_message_vector_coming_from_center.append(np.zeros(128))           \n",
        "            mut[\"w_n_log_prob\"] = neighbor_w_log_probs\n",
        "            mut[\"m_n_log_prob\"] = neighbor_m_log_probs\n",
        "            mut[\"neighbor_aa_identities\"] = neighbor_aa_identities\n",
        "            mut[\"neighbor_w_message_vector_coming_from_center\"] = neighbor_w_message_vector_coming_from_center\n",
        "            mut[\"neighbor_m_message_vector_coming_from_center\"] = neighbor_m_message_vector_coming_from_center\n",
        "            mut[\"neighbor_w_neighbor_embedding\"] = neighbor_w_embedding_info\n",
        "            mut[\"neighbor_m_neighbor_embedding\"] = neighbor_m_embedding_info\n",
        "        prot_end = time.time()\n",
        "        print(f\"Took {prot_end-prot_start} for {filename} with {mut_track+1} forward-mutations\")\n",
        "        print(\"....................\")"
      ],
      "metadata": {
        "id": "b8cEsTK1EQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the incomplete \"two_level_dict\" as pickle file for a quick dirty comparison\n",
        "# import pickle\n",
        "# with open(\"S_669_pmppn_info_dict_V3.pickle\",\"wb\") as f:\n",
        "#     pickle.dump(two_level_dict,f)"
      ],
      "metadata": {
        "id": "rXvc5PDmIQyF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle\n",
        "# with open(\"S_669_pmppn_info_dict_V2.pickle\",\"rb\") as f:\n",
        "#     two_level_dict = pickle.load(f)"
      ],
      "metadata": {
        "id": "T8iPDaqrZg4U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import entropy\n",
        "from scipy.special import expit\n",
        "alpha_list = list(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
        "# The following dictionary will be used for fetching out the log-probabilities corresponding to the wild-type and mutated residues at the mutation positions\n",
        "aa_to_N = {a:n for n,a in enumerate(alpha_list)}\n",
        "# This list will contain the experimental ddg values for the mutations for which two-level dict contains information regarding log_probabilities\n",
        "true_vals = []\n",
        "# This list will contain (wild_proba,mut_proba) tuples for the mutations for which two-level dict contains information regarding log_probabilities\n",
        "wild_mut_log_probabilities = []\n",
        "# saving max probabilites for debugging\n",
        "max_log_probabilities = []\n",
        "# Want to add entropy of the position with some kind of weight (maybe, just a for loop for checking weight combinations that sum to 1?)\n",
        "position_entropies = []\n",
        "weighted_neighbor_entropies = []\n",
        "# This \"weighted_neighbor_energy_changes\" will be ((-log(m))-(-log(w))) for each of the topk neighbors, weighted summed by corresponding attention weights   \n",
        "weighted_neighbor_energy_changes = []\n",
        "#\n",
        "backward_weighted_neighbor_energy_changes = []\n",
        "V2_backward_weighted_neighbor_energy_changes = []\n",
        "# the neighbor-forward-KL will at this point treat the center-wildtype conditioned neighbor distributions as true, \n",
        "# and center-mutated conditioned neighbor distributions as approximation\n",
        "weighted_neighbor_forward_KL = []\n",
        "# the neighbor-backward-KL will treat the center-mutated conditioned neighbor distributions as true, \n",
        "# and center-wildtype conditioned neighbor distributions as approximation\n",
        "weighted_neighbor_backward_KL = []\n",
        "#\n",
        "backward_weighted_neighbor_backward_KL = []\n",
        "V2_backward_weighted_neighbor_backward_KL = []  \n",
        "# # This \"weighted_neighbor_entropy_changes\" will be ((entropy(p(nieghbor|m))-(entropy(p(nieghbor|w))) for each of the topk neighbors, weighted summed by corresponding attention weights \n",
        "weighted_neighbor_entropy_changes = []\n",
        "## the weights will now be based on change in center->neighbor message vector L2-norm due to mutation, instead of neighbor->center message norm \n",
        "## however, I think a softmaxing across all those weights will now be a must since scales could be crazily different\n",
        "backward_weighted_neighbor_entropy_changes = []\n",
        "V2_backward_weighted_neighbor_entropy_changes = []\n",
        "# the two arrays below are being saved for checking correlations of (m/w) and (w/m) attention weight changes to ddgs\n",
        "# this debugging-type correlation analysis could reveal significant insights into the best weighing mechanism, and even provide a way to differentiate\n",
        "# between neighbors that have established new connections with the center, and neighbors whose connections with the center has been severed (kinda severed,maybe or weakened?..whatever)  \n",
        "center_neighbor_weight_check_m_w = []\n",
        "center_neighbor_weight_check_w_m = []\n",
        "\n",
        "# putting the dictionary here since we are going to need positions corresponding to alternate amino acid 1-letter codes\n",
        "alpha_tok = \"ACDEFGHIKLMNPQRSTVWYX\"\n",
        "aa_1_N = {a:n for n,a in enumerate(alpha_tok)}\n",
        "for i,(prot,muts) in enumerate(two_level_dict.items()):\n",
        "    if prot not in proteins_to_skip:\n",
        "        try:\n",
        "            cur_map_dict = mapping_dict[prot]\n",
        "        except:\n",
        "            continue\n",
        "        for ind_track, mut in enumerate(muts):\n",
        "            # only fetching those mutations that have corresponding log-probabilities calculated and saved as values of \"log_prob\" key\n",
        "            # where the fuck is \"log_prob\" coming from, but \"top_5_attention_weights\" and \"top_5_neighbor_indices\" are not there?\n",
        "            if (\"log_prob\" in mut) and (\"w_n_log_prob\" in mut):\n",
        "                wild = mut[\"mut\"][0] \n",
        "                alternate = mut[\"mut\"][-1]\n",
        "                true_vals.append(mut[\"ddg\"])\n",
        "                sequence_index_of_mutation = cur_map_dict[mut[\"mut\"][0:-1]]\n",
        "                position_log_probabilities = mut[\"log_prob\"][0,sequence_index_of_mutation,:]\n",
        "                wild_mut_log_probabilities.append((position_log_probabilities[aa_to_N[wild]],position_log_probabilities[aa_to_N[alternate]]))\n",
        "                max_log_probabilities.append(position_log_probabilities.max())\n",
        "                position_entropies.append(entropy(np.exp(position_log_probabilities)))\n",
        "                # These 0-based neighbor indices will be used for extracting the log-probabilities corresponding to the neighbor positions\n",
        "                n_indices= mut[\"top_15_neighbor_indices\"]\n",
        "                # The neighbor weights will be used here for multiplying \n",
        "                n_weights = mut[\"top_15_attention_weights\"].reshape(-1,1)\n",
        "                # Take entropy while taking care of the dimension along which entropy is calculated\n",
        "                # Taking entropy across last axis, because the shape of the input is (k,21), where 21 is the 21-way probability distribution \n",
        "                n_entropies = entropy(np.exp(mut[\"log_prob\"][0,n_indices,:]),axis=-1).reshape(-1,1)\n",
        "                # I think this element-wise product is getting wrong \n",
        "                weighted_neighbor_entropies.append((n_entropies*softmax(n_weights)).sum())\n",
        "                # weighted_neighbor_entropies.append((n_entropies*softmax(n_weights)).sum())\n",
        "                # weighted_neighbor_entropies.append((n_entropies*n_weights).sum())\n",
        "\n",
        "                # Now, calculate neighbor energy changes, and then weighted sum them after extracting specific log_probabilities\n",
        "                # for mutant center, and wildtype center impacted versions for the top neighbor positions\n",
        "                neighbor_w_log_probabilities = mut[\"w_n_log_prob\"]\n",
        "                neighbor_m_log_probabilities = mut[\"m_n_log_prob\"]\n",
        "                neighbor_amino_a_identities = mut[\"neighbor_aa_identities\"]\n",
        "                neighbor_w_message_vector_coming_from_center = mut[\"neighbor_w_message_vector_coming_from_center\"]\n",
        "                neighbor_m_message_vector_coming_from_center = mut[\"neighbor_m_message_vector_coming_from_center\"]\n",
        "                neighbor_w_neighbor_embedding = mut[\"neighbor_w_neighbor_embedding\"]\n",
        "                neighbor_m_neighbor_embedding = mut[\"neighbor_m_neighbor_embedding\"]\n",
        "                # The \"local_neighbor_log_prob_vals\" will be a list of negative log-probability differences(a.k.a. energy differences)\n",
        "                local_neighbor_log_prob_vals = []\n",
        "                local_neighbor_forward_KL_vals = []\n",
        "                local_neighbor_backward_KL_vals = []\n",
        "                local_neighbor_entropy_change_vals = []\n",
        "                local_neighbor_attention_change_vals = []\n",
        "                # the two lists below are for debugging, and insight-revelation purposes, mostly\n",
        "                local_center_neighbor_weight_check_m_w = []\n",
        "                local_center_neighbor_weight_check_w_m = []\n",
        "                # the \"local_neighbor_embedding_changes\" list below will hold (neighbor_m_embedding/neighbor_w_embedding) arrays of length 128\n",
        "                # so, technically, this list can be converted to a (15,128) 2D numpy array\n",
        "                # all those (15,128) numpy arrays will later be concatenated across axis-0 before taking the PCA in the next cell\n",
        "                local_neighbor_embedding_changes = []\n",
        "                local_neighbor_embedding_changes_raw = []\n",
        "                local_neighbor_message_changes_raw = []\n",
        "                # the vector below is very similar to (w/m), but L2 norm is taken on the difference vector\n",
        "                local_center_neighbor_message_change_diff = []\n",
        "                # For example, selecting the numbers from the first 5 iterations of this loop will give neighbor energy change corresponding to the first 5 neighbors\n",
        "                for neighbor_w, neighbor_m, neighbor_aa, neighbor_w_message, neighbor_m_message, neighbor_w_embedding, neighbor_m_embedding  in \\\n",
        "                zip(neighbor_w_log_probabilities,neighbor_m_log_probabilities,neighbor_amino_a_identities,neighbor_w_message_vector_coming_from_center,neighbor_m_message_vector_coming_from_center,\n",
        "                    neighbor_w_neighbor_embedding,neighbor_m_neighbor_embedding):\n",
        "                    # get the amino acid identity for the neighbor position, run it through the mapping dictionary, get the log probabilities from those positions,\n",
        "                    # \"neighbor_w\" and \"neighbor_m\" arrays will directly give the log probabilities that need to be substracted to get the energy (put (-1) before thoese numbers?...think a bit)\n",
        "                    neighbor_index = aa_1_N[neighbor_aa]\n",
        "                    local_neighbor_log_prob_vals.append((-1*neighbor_m[neighbor_index])-(-1*neighbor_w[neighbor_index]))\n",
        "                    # summing the output of \"kl_div\", because one number comes for every positions in the currently processing neighbor distribution,\n",
        "                    # and I want to take the total deviation in that distribution \n",
        "                    local_neighbor_forward_KL_vals.append(kl_div(np.exp(neighbor_w),np.exp(neighbor_m)).sum())\n",
        "                    local_neighbor_backward_KL_vals.append(kl_div(np.exp(neighbor_m),np.exp(neighbor_w)).sum())\n",
        "                    local_neighbor_entropy_change_vals.append(entropy(np.exp(neighbor_m))-entropy(np.exp(neighbor_w)))\n",
        "                    # lets just put the absolute value of the difference between the L2-norms for now (taking sign into consideration will require\n",
        "                    # taking care of the correct direction of change, which might make things a bit ncomplicated for now, and have unintended effects)\n",
        "                    # But, having a direction can definitely help with stabilization vs. de-stabilization figuring out\n",
        "                    # some kind of \n",
        "                    # usiung only the norm would more like predict the magnitude of DDG\n",
        "                    # local_neighbor_attention_change_vals.append()\n",
        "                    neighbor_w_message = neighbor_w_message.reshape((-1,1))\n",
        "                    neighbor_m_message = neighbor_m_message.reshape((-1,1))\n",
        "                    # Adding the small numbers for numerical stability, which hopefully will not change the information content of these features\n",
        "                    neighbor_w_message_norm = np.linalg.norm(neighbor_w_message,ord=2,axis=0) + 0.00000001\n",
        "                    neighbor_m_message_norm = np.linalg.norm(neighbor_m_message,ord=2,axis=0) + 0.00000001\n",
        "                    # taking average of two ratios for kind of capturing the change in both (mutant->wild) and (wild->mutant) directions with the same measure \n",
        "                    # the ratio should help to avoid any neighbor to neighbor variability related-scale\n",
        "                    # but scale could also be important, right? since, the same transformation is applied for calculating the numbers for every position in the proteins \n",
        "                    # local_neighbor_attention_change_vals.append((0.5*(neighbor_w_message_norm/neighbor_m_message_norm))+(0.5*(neighbor_m_message_norm/neighbor_w_message_norm)))\n",
        "                    local_neighbor_attention_change_vals.append((neighbor_w_message_norm/neighbor_m_message_norm))\n",
        "                    # the two list-appending lines below are mostly for debugging purposes at the moment,\n",
        "                    # might be something more than that in a few minutes?\n",
        "                    local_center_neighbor_weight_check_m_w.append(neighbor_m_message_norm/neighbor_w_message_norm)\n",
        "                    local_center_neighbor_weight_check_w_m.append(neighbor_w_message_norm/neighbor_m_message_norm)\n",
        "                    # can take PCA of the below as well\n",
        "                    # currently, taking the norm of division vector (-1 to discourage (+1) from contributing to the norm)\n",
        "                    # local_neighbor_embedding_changes.append(np.linalg.norm((neighbor_m_embedding/neighbor_w_embedding)))\n",
        "                    # local_neighbor_embedding_changes.append(neighbor_m_embedding/neighbor_w_embedding)\n",
        "                    # local_neighbor_embedding_changes.append(neighbor_w_embedding)\n",
        "                    local_neighbor_embedding_changes.append(np.linalg.norm(neighbor_w_embedding-neighbor_m_embedding))\n",
        "                    local_neighbor_embedding_changes_raw.append(neighbor_w_embedding-neighbor_m_embedding)\n",
        "                    local_neighbor_message_changes_raw.append(neighbor_w_message-neighbor_m_message)\n",
        "                    local_center_neighbor_message_change_diff.append(np.linalg.norm(neighbor_w_message-neighbor_m_message))\n",
        "                # The energy change approximation can be constrained to the top few neighbors by just indexing the arrays below\n",
        "                # So, it looks like a better idea to save log_probs for atleast the top_20 neighbors since we can always fetch the first few from there\n",
        "                weighted_neighbor_energy_changes.append((np.array(local_neighbor_log_prob_vals[0:15])*expit(n_weights[0:15])).sum())\n",
        "                weighted_neighbor_forward_KL.append((np.array(local_neighbor_forward_KL_vals[0:15])*expit(n_weights[0:15])).sum())\n",
        "                weighted_neighbor_backward_KL.append((np.array(local_neighbor_backward_KL_vals[0:15])*expit(n_weights[0:15])).sum())\n",
        "                weighted_neighbor_entropy_changes.append((np.array(local_neighbor_entropy_change_vals[0:15])*expit(n_weights[0:15])).sum())\n",
        "                # versions of neighbor features, just weighted by backward weights (center->neighbor attention change due to mutation) instead of forward weights (neighbor->center attention)\n",
        "                # sum() of these (m/w) weight-change is apparently positively correlated with DDGs\n",
        "                backward_weighted_neighbor_energy_changes.append((np.array(local_neighbor_log_prob_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_m_w))[0:15])).sum())\n",
        "                backward_weighted_neighbor_backward_KL.append((np.array(local_neighbor_backward_KL_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_m_w))[0:15])).sum())\n",
        "                backward_weighted_neighbor_entropy_changes.append(((np.array(local_neighbor_entropy_change_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_m_w)))[0:15]).sum()))\n",
        "                # the two list-appending lines below are mostly for debugging purposes at the moment,\n",
        "                # might be something more than that in a few minutes?\n",
        "                center_neighbor_weight_check_m_w.append(np.array(local_center_neighbor_weight_check_m_w).sum())\n",
        "                center_neighbor_weight_check_w_m.append(np.array(local_center_neighbor_weight_check_w_m).sum())\n",
        "                # now, let us add features weighted by the other version of attention changes\n",
        "                # sum() of these (w/m) weight-change is apparently negatievly correlated with DDGs\n",
        "                V2_backward_weighted_neighbor_energy_changes.append((np.array(local_neighbor_log_prob_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_w_m))[0:15])).sum())\n",
        "                V2_backward_weighted_neighbor_backward_KL.append((np.array(local_neighbor_backward_KL_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_w_m))[0:15])).sum())\n",
        "                V2_backward_weighted_neighbor_entropy_changes.append(((np.array(local_neighbor_entropy_change_vals[0:15])*expit((np.array(local_center_neighbor_weight_check_w_m)))[0:15]).sum()))\n",
        "\n",
        "\n",
        "                # let us also save the features in the \"mut\" dictionary, so that \"two_letter_dict\" containing all the features for every mutation\n",
        "                # can be saved after this cell in pickle format, and later be accessed from any other script\n",
        "                # print(pearsonr(experimental_energies,mut_wild_predictions))\n",
        "                # print(pearsonr(experimental_energies,mut_min_predictions))\n",
        "                # energies (negative log probabilities of the most_probable,wild,and mutated residue, respectively, at the center position)\n",
        "                e_max = (-1*(max_log_probabilities[-1]))\n",
        "                e_wild = (-1*((wild_mut_log_probabilities[-1])[0]))\n",
        "                e_mut = (-1*((wild_mut_log_probabilities[-1])[1]))\n",
        "                mut[\"center_mut_wild_energy\"] = e_mut - e_wild \n",
        "                mut[\"center_mut_max_energy\"] = e_mut - e_max\n",
        "                mut[\"center_entropy\"] = (position_entropies[-1] * (-1))\n",
        "                mut[\"weighted_neighbor_entropies\"] = weighted_neighbor_entropies[-1] \n",
        "                mut[\"weighted_neighbor_energy_changes\"] = weighted_neighbor_energy_changes[-1]\n",
        "                mut[\"backward_weighted_neighbor_energy_changes\"] = backward_weighted_neighbor_energy_changes[-1]\n",
        "                mut[\"V2_backward_weighted_neighbor_energy_changes\"] = V2_backward_weighted_neighbor_energy_changes[-1]\n",
        "                mut[\"weighted_neighbor_forward_KL\"] = weighted_neighbor_forward_KL[-1] \n",
        "                mut[\"weighted_neighbor_backward_KL\"] = weighted_neighbor_backward_KL[-1]\n",
        "                mut[\"backward_weighted_neighbor_backward_KL\"] = backward_weighted_neighbor_backward_KL[-1]\n",
        "                mut[\"V2_backward_weighted_neighbor_backward_KL\"] = V2_backward_weighted_neighbor_backward_KL[-1]  \n",
        "                mut[\"weighted_neighbor_entropy_changes\"] = weighted_neighbor_entropy_changes[-1]\n",
        "                mut[\"backward_weighted_neighbor_entropy_changes\"] = backward_weighted_neighbor_entropy_changes[-1]\n",
        "                mut[\"V2_backward_weighted_neighbor_entropy_changes\"] = V2_backward_weighted_neighbor_entropy_changes[-1]  \n",
        "                mut[\"center_neighbor_weight_check_m_w\"] = center_neighbor_weight_check_m_w[-1]\n",
        "                mut[\"center_neighbor_weight_check_w_m\"] = center_neighbor_weight_check_w_m[-1]\n",
        "                # mut[\"neighbor_embedding_change_m_w\"] should be a (15,128) numpy array for every mutation since we are currently \n",
        "                # considering top15 attended neighbors for every mutation, and the decoder last layer embedding before \n",
        "                # output_transformation is of length 128 \n",
        "                # mut[\"neighbor_embedding_change_m_w\"] = np.array(local_neighbor_embedding_changes)\n",
        "                mut[\"neighbor_embedding_change_m_w\"] = np.array(local_neighbor_embedding_changes).sum()\n",
        "                # for each mutation, mut[\"neighbor_embedding_change_m_w_raw\"] should be a (15,128) array since we are considering\n",
        "                # top15 most attended neighbors \n",
        "                mut[\"neighbor_embedding_change_m_w_raw\"] = np.array(local_neighbor_embedding_changes_raw)\n",
        "                mut[\"neighbor_message_change_m_w_raw\"] = np.array(local_neighbor_message_changes_raw)\n",
        "                mut[\"neighbor_message_change_m_w\"] = np.array(local_center_neighbor_message_change_diff).sum()\n",
        "                mut[\"unweighted_backward_KL\"] = np.array(local_neighbor_backward_KL_vals).sum()\n",
        "                mut[\"unweighted_forward_KL\"] = np.array(local_neighbor_forward_KL_vals).sum()"
      ],
      "metadata": {
        "id": "7m4jsITsJHmE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "TwMx2ZMs60Pz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experimental_energies = []\n",
        "mut_wild_predictions = []\n",
        "mut_min_predictions = []\n",
        "entropy_predictions = []\n",
        "neighbor_entropy_change_predictions = []\n",
        "neighbor_energy_change_predictions = []\n",
        "center_neighbor_weight_check_w_m = []\n",
        "V2_backward_weighted_neighbor_backward_KL = []\n",
        "sum_neighbor_embedding_change_m_w = []\n",
        "neighbor_message_change_m_w = []\n",
        "\n",
        "for prot,muts in two_level_dict.items():\n",
        "    if prot not in proteins_to_skip:\n",
        "        for mut in muts:\n",
        "            if \"center_mut_wild_energy\" in mut:\n",
        "                experimental_energies.append(mut[\"ddg\"])\n",
        "                mut_wild_predictions.append(mut[\"center_mut_wild_energy\"])\n",
        "                mut_min_predictions.append(mut[\"center_mut_max_energy\"])\n",
        "                entropy_predictions.append(mut[\"center_entropy\"])\n",
        "                neighbor_entropy_change_predictions.append(mut[\"weighted_neighbor_entropy_changes\"])\n",
        "                neighbor_energy_change_predictions.append(mut[\"weighted_neighbor_energy_changes\"])\n",
        "                center_neighbor_weight_check_w_m.append(mut[\"center_neighbor_weight_check_w_m\"])\n",
        "                V2_backward_weighted_neighbor_backward_KL.append(mut[\"V2_backward_weighted_neighbor_backward_KL\"])\n",
        "                sum_neighbor_embedding_change_m_w.append(mut[\"neighbor_embedding_change_m_w\"])\n",
        "                neighbor_message_change_m_w.append(mut[\"neighbor_message_change_m_w\"])\n",
        "\n",
        "print(len(experimental_energies),len(mut_wild_predictions))\n",
        "\n",
        "print(pearsonr(experimental_energies,mut_wild_predictions))\n",
        "print(pearsonr(experimental_energies,mut_min_predictions))\n",
        "print(pearsonr(experimental_energies,entropy_predictions))\n",
        "print(pearsonr(experimental_energies,neighbor_entropy_change_predictions))\n",
        "print(pearsonr(experimental_energies,neighbor_energy_change_predictions))\n",
        "print(pearsonr(experimental_energies,center_neighbor_weight_check_w_m))\n",
        "print(pearsonr(experimental_energies,V2_backward_weighted_neighbor_backward_KL))\n",
        "print(pearsonr(experimental_energies,sum_neighbor_embedding_change_m_w))\n",
        "print(pearsonr(experimental_energies,neighbor_message_change_m_w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StvbASTsfta0",
        "outputId": "a07d6975-2b7e-4ed3-a00e-1ee9266006ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "638 638\n",
            "(-0.3395853224754521, 1.1023372613588465e-18)\n",
            "(-0.28758653506044757, 1.2934271538127842e-13)\n",
            "(-0.2520013958449332, 1.0663549385817708e-10)\n",
            "(-0.18685355848977528, 2.0108330840296436e-06)\n",
            "(-0.21952531810806739, 2.1125347283157265e-08)\n",
            "(0.020920827570630865, 0.5978786475461585)\n",
            "(-0.3591958280077496, 7.287374296171487e-21)\n",
            "(-0.37297308523189143, 1.7289309061530068e-22)\n",
            "(-0.012043164989239888, 0.7614260125986811)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let me get PSSM values and do some comparison quickly\n",
        "# getting the PSSM extraction functions from my custom model data processing scripts\n",
        "from string import ascii_uppercase\n",
        "\n",
        "# In rare cases, some PDB files number chains with 1,2,3 instead of A,B,C\n",
        "def convertChainFromAlphabetToNumber(alphabet):\n",
        "    mappingDict = {ch:(idx+1) for idx,ch in enumerate(ascii_uppercase)}\n",
        "    return str(mappingDict[alphabet])\n",
        "\n",
        "# Before executing this function, the PSSM files with naming format \"pdbIdchain.pssm\" needs to be stored\n",
        "# in the pssm_dir\n",
        "def returnPSSMArray(pdbIdPlusChain,pssm_dir=\"train_pssm_dir\",convert_upper = False):\n",
        "#     Currently, assuming that the pssm file names contain pdbId in upper case\n",
        "    if convert_upper:\n",
        "        fileName = pdbIdPlusChain.upper() + \".pssm\"\n",
        "    else:\n",
        "        fileName = pdbIdPlusChain + \".pssm\"\n",
        "    try:\n",
        "        fullPath = os.path.join(pssm_dir,fileName)\n",
        "        f = open(fullPath)\n",
        "    except:\n",
        "        fileName = pdbIdPlusChain[0:4].upper() + str(convertChainFromAlphabetToNumber(pdbIdPlusChain[4])) + \".pssm\" \n",
        "        fullPath = os.path.join(pssm_dir,fileName)\n",
        "        f = open(fullPath)\n",
        "        \n",
        "# #     all the target lines in the PSSM files have (2+20+20+2=44) strings after line.split()\n",
        "    target_lines = [line.split() for line in f.readlines() if (len(line.split()))==44]\n",
        "    number_of_residues = len(target_lines)\n",
        "    \n",
        "    pssm_features = np.zeros((number_of_residues,20))\n",
        "\n",
        "    for idx,line in enumerate(target_lines):\n",
        "        pssm_features[idx,:] = line[2:22]\n",
        "\n",
        "    f.close()\n",
        "    \n",
        "    return pssm_features\n",
        "\n",
        "# This function also seems necessary for extracting the two pssm values\n",
        "# Must review the three pssm feature functions (this one and the two above) later\n",
        "# These functions seem to be taking up a lot of time....must review\n",
        "def returnPSSMMapping(residue):\n",
        "    pssm_letter_to_index_dict = {\"A\" : 0,   \n",
        "    \"R\" : 1,\n",
        "    \"N\" : 2,\n",
        "    \"D\" : 3,\n",
        "    \"C\" : 4,\n",
        "    \"Q\" : 5,\n",
        "    \"E\" : 6,\n",
        "    \"G\" : 7,\n",
        "    \"H\" : 8,\n",
        "    \"I\" : 9,\n",
        "    \"L\" : 10,\n",
        "    \"K\" : 11,\n",
        "    \"M\" : 12,\n",
        "    \"F\" : 13,\n",
        "    \"P\" : 14,\n",
        "    \"S\" : 15,\n",
        "    \"T\" : 16,\n",
        "    \"W\" : 17,\n",
        "    \"Y\" : 18,\n",
        "    \"V\" : 19}\n",
        "\n",
        "    return pssm_letter_to_index_dict[residue]\n",
        "\n",
        "\n",
        "# I will add PSSM values to the two-level dictionary for places where log_prob is available\n",
        "pssmDirectory = \"/content/drive/MyDrive/ACCRE_PyRun_Setup/S_669_pssm_dir\"\n",
        "for prot,muts in two_level_dict.items():\n",
        "    if prot not in proteins_to_skip:\n",
        "        try:\n",
        "            cur_map_dict = mapping_dict[prot]\n",
        "        except:\n",
        "            continue\n",
        "        for mut in muts:\n",
        "            # only fetching those mutations that have corresponding log-probabilities calculated and saved as values of \"log_prob\" key\n",
        "            if \"log_prob\" in mut:\n",
        "                wild = mut[\"mut\"][0] \n",
        "                alternate = mut[\"mut\"][-1]\n",
        "                sequence_index_of_mutation = cur_map_dict[mut[\"mut\"][0:-1]]\n",
        "                pdbId = prot[0:-1]\n",
        "                mutChain = prot[-1]\n",
        "                pssm_array = returnPSSMArray(pdbId + mutChain,pssm_dir=pssmDirectory,convert_upper = False)\n",
        "                position_pssm = pssm_array[sequence_index_of_mutation]\n",
        "                wild_pssm = position_pssm[returnPSSMMapping(wild)] \n",
        "                alternate_pssm = position_pssm[returnPSSMMapping(alternate)]\n",
        "                mut[\"wild_pssm\"] = wild_pssm\n",
        "                mut[\"alternate_pssm\"] = alternate_pssm"
      ],
      "metadata": {
        "id": "eBgpxQx4ispc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pssm_predictions = []\n",
        "for prot,muts in two_level_dict.items():\n",
        "    if prot not in proteins_to_skip:\n",
        "        try:\n",
        "            cur_map_dict = mapping_dict[prot]\n",
        "        except:\n",
        "            continue\n",
        "        for mut in muts:\n",
        "            # only fetching those mutations that have corresponding log-probabilities calculated and saved as values of \"log_prob\" key\n",
        "            if \"log_prob\" in mut:\n",
        "                pssm_predictions.append((mut[\"wild_pssm\"]-mut[\"alternate_pssm\"]))"
      ],
      "metadata": {
        "id": "xOkQhEQzn32m"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experimental_energies = []\n",
        "mut_wild_predictions = []\n",
        "mut_min_predictions = []\n",
        "entropy_predictions = []\n",
        "neighbor_entropy_change_predictions = []\n",
        "neighbor_energy_change_predictions = []\n",
        "center_neighbor_weight_check_w_m = []\n",
        "V2_backward_weighted_neighbor_backward_KL = []\n",
        "sum_neighbor_embedding_change_m_w = []\n",
        "neighbor_message_change_m_w = []\n",
        "\n",
        "for prot,muts in two_level_dict.items():\n",
        "    if prot not in proteins_to_skip:\n",
        "        for mut in muts:\n",
        "            if \"center_mut_wild_energy\" in mut:\n",
        "                experimental_energies.append(mut[\"ddg\"])\n",
        "                mut_wild_predictions.append(mut[\"center_mut_wild_energy\"])\n",
        "                mut_min_predictions.append(mut[\"center_mut_max_energy\"])\n",
        "                entropy_predictions.append(mut[\"center_entropy\"])\n",
        "                neighbor_entropy_change_predictions.append(mut[\"weighted_neighbor_entropy_changes\"])\n",
        "                neighbor_energy_change_predictions.append(mut[\"weighted_neighbor_energy_changes\"])\n",
        "                center_neighbor_weight_check_w_m.append(mut[\"center_neighbor_weight_check_w_m\"])\n",
        "                V2_backward_weighted_neighbor_backward_KL.append(mut[\"V2_backward_weighted_neighbor_backward_KL\"])\n",
        "                sum_neighbor_embedding_change_m_w.append(mut[\"neighbor_embedding_change_m_w\"])\n",
        "                neighbor_message_change_m_w.append(mut[\"neighbor_message_change_m_w\"])\n",
        "\n",
        "print(len(experimental_energies),len(mut_wild_predictions))\n",
        "\n",
        "print(pearsonr(experimental_energies,mut_wild_predictions))\n",
        "print(pearsonr(experimental_energies,mut_min_predictions))\n",
        "print(pearsonr(experimental_energies,entropy_predictions))\n",
        "print(pearsonr(experimental_energies,neighbor_entropy_change_predictions))\n",
        "print(pearsonr(experimental_energies,neighbor_energy_change_predictions))\n",
        "print(pearsonr(experimental_energies,center_neighbor_weight_check_w_m))\n",
        "print(pearsonr(experimental_energies,V2_backward_weighted_neighbor_backward_KL))\n",
        "print(pearsonr(experimental_energies,sum_neighbor_embedding_change_m_w))\n",
        "print(pearsonr(experimental_energies,neighbor_message_change_m_w))\n",
        "print(pearsonr(experimental_energies,pssm_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tyw_4erpj1C",
        "outputId": "e6992b1c-9ecc-43e8-fc4a-b47e92833bb0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "638 638\n",
            "(-0.3395853224754521, 1.1023372613588465e-18)\n",
            "(-0.28758653506044757, 1.2934271538127842e-13)\n",
            "(-0.2520013958449332, 1.0663549385817708e-10)\n",
            "(-0.18685355848977528, 2.0108330840296436e-06)\n",
            "(-0.21952531810806739, 2.1125347283157265e-08)\n",
            "(0.020920827570630865, 0.5978786475461585)\n",
            "(-0.3591958280077496, 7.287374296171487e-21)\n",
            "(-0.37297308523189143, 1.7289309061530068e-22)\n",
            "(-0.012043164989239888, 0.7614260125986811)\n",
            "(-0.1760175004428737, 7.741516443498937e-06)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df_Ssym = pd.DataFrame(\n",
        "\n",
        "    {'DDG' : experimental_energies,\n",
        "     'P_DP': mut_wild_predictions,\n",
        "     'P_ET': entropy_predictions,\n",
        "     'P_DEC': pssm_predictions,\n",
        "     'Neighbor_Energy_Change' : neighbor_energy_change_predictions,\n",
        "     'Neighbor_backward_KL' : V2_backward_weighted_neighbor_backward_KL,\n",
        "     'Neighbor_Entropy_Change' : neighbor_entropy_change_predictions,\n",
        "     'neighbor_attention_change_w/m' : center_neighbor_weight_check_w_m,\n",
        "     'neighbor_embedding_change' : sum_neighbor_embedding_change_m_w,\n",
        "     'neighbor_message_change' : neighbor_message_change_m_w  \n",
        "    })\n",
        "corr = df_Ssym.corr()\n",
        "\n",
        "sns.set(font_scale=1.4)\n",
        "sns.heatmap(corr, \n",
        "        xticklabels=corr.columns,\n",
        "        yticklabels=corr.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "QlQuGW7f7KDy",
        "outputId": "1835feab-992e-45dc-8576-cda8bdba7596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9c98be2190>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}